{"cells":[{"cell_type":"code","execution_count":42,"id":"1778e57a","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17856,"status":"ok","timestamp":1725921390706,"user":{"displayName":"Aditya Singhal","userId":"02233461186993627837"},"user_tz":240},"id":"1778e57a","outputId":"0385b874-14ce-4494-f175-62f09c71ab9b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: emoji in c:\\users\\adity\\anaconda3\\lib\\site-packages (2.12.1)\n","Requirement already satisfied: typing-extensions>=4.7.0 in c:\\users\\adity\\anaconda3\\lib\\site-packages (from emoji) (4.12.2)\n","Requirement already satisfied: nx2vos in c:\\users\\adity\\anaconda3\\lib\\site-packages (0.2)\n","Requirement already satisfied: networkx>=3.0 in c:\\users\\adity\\anaconda3\\lib\\site-packages (from nx2vos) (3.1)\n","Requirement already satisfied: python-louvain in c:\\users\\adity\\anaconda3\\lib\\site-packages (0.16)\n","Requirement already satisfied: networkx in c:\\users\\adity\\anaconda3\\lib\\site-packages (from python-louvain) (3.1)\n","Requirement already satisfied: numpy in c:\\users\\adity\\anaconda3\\lib\\site-packages (from python-louvain) (1.24.3)\n","Requirement already satisfied: nx2vos in c:\\users\\adity\\anaconda3\\lib\\site-packages (0.2)\n","Requirement already satisfied: networkx>=3.0 in c:\\users\\adity\\anaconda3\\lib\\site-packages (from nx2vos) (3.1)\n"]}],"source":["# Install Libraries\n","!pip install emoji\n","!pip install nx2vos\n","!pip install python-louvain\n","!pip install nx2vos"]},{"cell_type":"code","execution_count":43,"id":"1ab5b00a","metadata":{"executionInfo":{"elapsed":2284,"status":"ok","timestamp":1725921392980,"user":{"displayName":"Aditya Singhal","userId":"02233461186993627837"},"user_tz":240},"id":"1ab5b00a"},"outputs":[],"source":["# All Imports\n","import os\n","import io\n","import re\n","import csv\n","import time\n","import hashlib\n","import unicodedata\n","import json\n","import requests\n","from decimal import Decimal\n","from itertools import islice, combinations\n","from collections import deque, defaultdict, OrderedDict, namedtuple, Counter\n","from typing import Any, List, Dict, Set, Optional\n","from enum import Enum\n","from emoji import EMOJI_DATA\n","import networkx as nx\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","from pathlib import Path\n","import string\n","from math import factorial\n","from operator import itemgetter\n","from nx2vos import write_vos_json\n","import networkx as nx\n","from matplotlib.patches import FancyArrowPatch\n","from matplotlib.colors import to_rgba\n","import numpy as np\n","from community import community_louvain\n","from sklearn.manifold import MDS"]},{"cell_type":"code","execution_count":44,"id":"2a5d10d5","metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1725921392980,"user":{"displayName":"Aditya Singhal","userId":"02233461186993627837"},"user_tz":240},"id":"2a5d10d5"},"outputs":[],"source":["# StopWordsRemover and Stopwords\n","\n","class StopWordsRemover:\n","    def __init__(self, min_word_length: int, lang: str):\n","        self.min_word_length = min_word_length\n","        self.lang = lang\n","        self.max_accepted_garbage = 3\n","        self.nb_stop_words = 5000\n","        self.nb_stop_words_short = 500\n","\n","        self.set_stop_words_field_specific_or_short: Set[str] = set()\n","        self.set_stop_words_short: Set[str] = set()\n","        self.set_stopwords_field_specific: Set[str] = set()\n","        self.set_stop_words: Set[str] = set()\n","        self.set_keep_words: Set[str] = set()\n","        self.set_remove_words: Set[str] = set()\n","\n","        stop_words_long_and_short = Stopwords.get_stop_words(lang)\n","        self.stopwords_long = list(stop_words_long_and_short.get(\"long\", []))\n","        self.nb_stop_words_short = min(self.nb_stop_words_short, max(0, len(self.stopwords_long) - 1))\n","        self.nb_stop_words = min(5000, max(0, len(self.stopwords_long) - 1))\n","\n","        try:\n","            self.init()\n","        except Exception as ex:\n","            print(f\"Exception: {ex}\")\n","\n","    def add_stop_words_to_keep(self, words_to_keep: Set[str]):\n","        if words_to_keep:\n","            self.set_keep_words.update(words_to_keep)\n","\n","    def add_words_to_remove(self, words_to_remove: Set[str]):\n","        if words_to_remove:\n","            self.set_remove_words.update(words_to_remove)\n","\n","    def use_user_supplied_stopwords(self, user_supplied_stopwords: Set[str], user_stopwords_replace_default: bool):\n","        if user_stopwords_replace_default:\n","            self.set_stop_words_field_specific_or_short = set(user_supplied_stopwords)\n","            self.set_stop_words_short = set(user_supplied_stopwords)\n","            self.set_stopwords_field_specific = set(user_supplied_stopwords)\n","            self.set_stop_words = set(user_supplied_stopwords)\n","        else:\n","            self.set_stop_words_field_specific_or_short.update(user_supplied_stopwords)\n","            self.set_stop_words_short.update(user_supplied_stopwords)\n","            self.set_stop_words.update(user_supplied_stopwords)\n","\n","    def add_field_specific_stop_words(self, field_specific_stop_words_to_remove: Set[str]):\n","        if field_specific_stop_words_to_remove:\n","            self.set_stop_words_field_specific_or_short.update(field_specific_stop_words_to_remove)\n","            self.set_stop_words.update(field_specific_stop_words_to_remove)\n","\n","    def init(self):\n","        self.set_keep_words = set()\n","        self.set_stop_words_short = set()\n","\n","        list_general_stopwords_large = self.stopwords_long[:self.nb_stop_words]\n","        list_general_stopwords_short = self.stopwords_long[:self.nb_stop_words_short]\n","\n","        self.set_stop_words.update(list_general_stopwords_large)\n","        self.set_stop_words.update(Stopwords.get_stopwords_valid_for_all_languages())\n","\n","        short_words = Stopwords.get_stop_words(self.lang).get(\"short\", [])\n","        if not short_words:\n","            self.set_stop_words_short.update(list_general_stopwords_short)\n","        else:\n","            self.set_stop_words_short.update(short_words)\n","\n","        self.set_stop_words_field_specific_or_short.update(self.set_stop_words_short)\n","\n","    def should_it_be_removed(self, term: str) -> bool:\n","        entry_word = term\n","        multiple_word = \" \" in entry_word\n","\n","        write = True\n","\n","        if multiple_word:\n","            words_ngrams = entry_word.split(\" \")\n","            words_ngrams_length = len(words_ngrams)\n","\n","            for words_ngram in words_ngrams:\n","                if len(words_ngram) < self.min_word_length:\n","                    write = False\n","                    break\n","\n","            if words_ngrams_length == 2 and (\n","                words_ngrams[0].lower().strip() in self.set_stop_words_field_specific_or_short or\n","                words_ngrams[1].lower().strip() in self.set_stop_words_field_specific_or_short\n","            ):\n","                write = False\n","\n","            if words_ngrams_length > 2:\n","                score_garbage = 0\n","\n","                for i, current_term in enumerate(words_ngrams):\n","                    current_term = current_term.lower().strip()\n","\n","                    if (i == 0 or i == (words_ngrams_length - 1)) and current_term in self.set_stop_words_field_specific_or_short:\n","                        score_garbage = self.max_accepted_garbage + 1\n","                        continue\n","\n","                    if (i == 0 or i == (words_ngrams_length - 1)) and current_term in self.set_stop_words_short:\n","                        write = False\n","                        continue\n","\n","                    if current_term in self.set_stop_words_short:\n","                        score_garbage += 3\n","                        continue\n","\n","                    if current_term in self.set_stopwords_field_specific:\n","                        score_garbage += 2\n","                        continue\n","\n","                if entry_word in self.set_stop_words:\n","                    score_garbage = self.max_accepted_garbage + 1\n","\n","                if score_garbage > self.max_accepted_garbage:\n","                    write = False\n","\n","        elif entry_word in self.set_stop_words and entry_word not in self.set_keep_words:\n","            write = False\n","\n","        if entry_word in self.set_keep_words:\n","            write = True\n","        if entry_word in self.set_remove_words:\n","            write = False\n","\n","        return not write\n"]},{"cell_type":"code","execution_count":45,"id":"56e45d41","metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1725921392980,"user":{"displayName":"Aditya Singhal","userId":"02233461186993627837"},"user_tz":240},"id":"56e45d41"},"outputs":[],"source":["import os\n","from pathlib import Path\n","\n","class Stopwords:\n","    twitter_stop_words = {\"rt\", \"w/\"}\n","    common_stop_words = {\"and\", \"for\", \"nbsp\", \"http\", \"https\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"10\", \"20\", \"25\", \"30\", \"40\", \"50\", \"100\", \"1000\"}\n","\n","    cache = {}\n","    cache_twitter = {}\n","\n","    @staticmethod\n","    def get_stop_words(lang):\n","        if lang in Stopwords.cache:\n","            return Stopwords.cache[lang]\n","\n","        stop_words = set(Stopwords.common_stop_words)\n","        short_stop_words = set(Stopwords.common_stop_words)\n","\n","        stop_words.update(Stopwords.twitter_stop_words)\n","        short_stop_words.update(Stopwords.twitter_stop_words)\n","\n","        path_locale = Stopwords.get_resource_path()\n","\n","        path_resource = Path(path_locale) / f\"{lang}.txt\"\n","        if path_resource.exists():\n","            with path_resource.open('r', encoding='utf-8') as file:\n","                stop_words.update(line.strip() for line in file)\n","\n","        path_resource = Path(path_locale) / f\"{lang}_short.txt\"\n","        if path_resource.exists():\n","            with path_resource.open('r', encoding='utf-8') as file:\n","                short_stop_words.update(line.strip() for line in file)\n","\n","        pair = {\"short\": short_stop_words, \"long\": stop_words}\n","        Stopwords.cache[lang] = pair\n","\n","        return pair\n","\n","    @staticmethod\n","    def get_stop_words_useful_in_sentiment_analysis(lang):\n","        stop_words = set()\n","        path_locale = Stopwords.get_resource_path()\n","\n","        path_resource = Path(path_locale) / f\"{lang}_stopword_sentiment.txt\"\n","        if path_resource.exists():\n","            with path_resource.open('r', encoding='utf-8') as file:\n","                stop_words.update(line.strip() for line in file)\n","\n","        return stop_words\n","\n","    @staticmethod\n","    def get_scientific_stopwords_in_english():\n","        return Stopwords._get_scientific_stopwords('scientificstopwords_en.txt')\n","\n","    @staticmethod\n","    def get_scientific_stopwords_in_french():\n","        return Stopwords._get_scientific_stopwords('scientificstopwords_fr.txt')\n","\n","    @staticmethod\n","    def _get_scientific_stopwords(filename):\n","        stop_words = set()\n","        path_locale = Stopwords.get_resource_path()\n","\n","        path_resource = Path(path_locale) / filename\n","        if path_resource.exists():\n","            with path_resource.open('r', encoding='utf-8') as file:\n","                stop_words.update(line.strip() for line in file)\n","\n","        return stop_words\n","\n","    @staticmethod\n","    def get_twitter_stopwords(long_list=True):\n","        words = set()\n","        path_locale = Stopwords.get_resource_path()\n","\n","        if long_list:\n","            path_resource = Path(path_locale) / \"twitter_long.txt\"\n","        else:\n","            path_resource = Path(path_locale) / \"twitter_short.txt\"\n","\n","        if path_resource.exists():\n","            with path_resource.open('r', encoding='utf-8') as file:\n","                words.update(line.strip() for line in file)\n","\n","        return words\n","\n","    @staticmethod\n","    def get_stopwords_valid_for_all_languages():\n","        words = set(Stopwords.common_stop_words)\n","        words.update(Stopwords.twitter_stop_words)\n","\n","        path_locale = Stopwords.get_resource_path()\n","\n","        path_resource = Path(path_locale) / \"stopwords_all_languages.txt\"\n","        if path_resource.exists():\n","            with path_resource.open('r', encoding='utf-8') as file:\n","                words.update(line.strip() for line in file)\n","\n","        return words\n","\n","    @staticmethod\n","    def get_resource_path():\n","        # Adjust this path according to your project's structure\n","\n","        return f\"drive/MyDrive/Colab Notebooks/Functions App/text_files/stopwords/\"\n","\n","class StopWordsRemover:\n","    def __init__(self, min_word_length, lang):\n","        self.min_word_length = min_word_length\n","        self.max_accepted_garbage = 3\n","        self.nb_stop_words = 5000\n","        self.nb_stop_words_short = 500\n","\n","        self.set_stop_words_field_specific_or_short = set()\n","        self.set_stop_words_short = set()\n","        self.set_stopwords_field_specific = set()\n","        self.set_stop_words = set()\n","        self.set_keep_words = set()\n","        self.set_remove_words = set()\n","        self.list_general_stopwords_large = []\n","        self.list_general_stopwords_short = []\n","        self.stopwords_long = []\n","        self.stop_words_long_and_short = Stopwords.get_stop_words(lang)\n","\n","        self.init()\n","\n","    def add_stop_words_to_keep(self, words_to_keep):\n","        if words_to_keep is not None:\n","            self.set_keep_words.update(words_to_keep)\n","\n","    def add_words_to_remove(self, words_to_remove):\n","        if words_to_remove is not None:\n","            self.set_remove_words.update(words_to_remove)\n","\n","    def use_user_supplied_stopwords(self, user_supplied_stopwords, user_stopwords_replace_default):\n","        if user_stopwords_replace_default:\n","            self.set_stop_words_field_specific_or_short = set(user_supplied_stopwords)\n","            self.set_stop_words_short = set(user_supplied_stopwords)\n","            self.set_stopwords_field_specific = set(user_supplied_stopwords)\n","            self.set_stop_words = set(user_supplied_stopwords)\n","        else:\n","            self.set_stop_words_field_specific_or_short.update(user_supplied_stopwords)\n","            self.set_stop_words_short.update(user_supplied_stopwords)\n","            self.set_stop_words.update(user_supplied_stopwords)\n","\n","    def add_field_specific_stop_words(self, field_specific_stop_words_to_remove):\n","        if field_specific_stop_words_to_remove is not None:\n","            self.set_stop_words_field_specific_or_short.update(field_specific_stop_words_to_remove)\n","            self.set_stop_words.update(field_specific_stop_words_to_remove)\n","\n","    def init(self):\n","        self.set_keep_words = set()\n","        self.set_stop_words_short = set()\n","\n","        self.list_general_stopwords_large = list(self.stop_words_long_and_short[\"long\"])[:self.nb_stop_words]\n","        self.list_general_stopwords_short = list(self.stop_words_long_and_short[\"short\"])[:self.nb_stop_words_short]\n","\n","        self.set_stop_words.update(self.list_general_stopwords_large)\n","        self.set_stop_words.update(Stopwords.get_stopwords_valid_for_all_languages())\n","        if not self.stop_words_long_and_short['short']:\n","            self.set_stop_words_short.update(self.list_general_stopwords_short)\n","        else:\n","            self.set_stop_words_short.update(self.stop_words_long_and_short['short'])\n","        self.set_stop_words_field_specific_or_short.update(self.set_stop_words_short)\n","\n","    def should_it_be_removed(self, term):\n","        entry_word = term\n","        write = True\n","        words_ngram = entry_word.split(\" \")\n","        multiple_word = len(words_ngram) > 1\n","\n","        if multiple_word:\n","            if any(len(word) < self.min_word_length for word in words_ngram):\n","                write = False\n","\n","            if len(words_ngram) == 2:\n","                if words_ngram[0].lower().strip() in self.set_stop_words_field_specific_or_short or words_ngram[1].lower().strip() in self.set_stop_words_field_specific_or_short:\n","                    write = False\n","\n","            if len(words_ngram) > 2:\n","                score_garbage = 0\n","                for i, current_term in enumerate(words_ngram):\n","                    current_term = current_term.lower().strip()\n","                    if (i == 0 or i == len(words_ngram) - 1) and current_term in self.set_stop_words_field_specific_or_short:\n","                        score_garbage += self.max_accepted_garbage + 1\n","                        continue\n","\n","                    if (i == 0 or i == len(words_ngram) - 1) and current_term in self.set_stop_words_short:\n","                        write = False\n","                        continue\n","\n","                    if current_term in self.set_stop_words_short:\n","                        score_garbage += 3\n","                        continue\n","\n","                    if current_term in self.set_stopwords_field_specific:\n","                        score_garbage += 2\n","                        continue\n","\n","                if entry_word in self.set_stop_words:\n","                    score_garbage = self.max_accepted_garbage + 1\n","\n","                if score_garbage > self.max_accepted_garbage:\n","                    write = False\n","\n","        elif entry_word in self.set_stop_words and entry_word not in self.set_keep_words:\n","            write = False\n","\n","        if entry_word in self.set_keep_words:\n","            write = True\n","        if entry_word in self.set_remove_words:\n","            write = False\n","\n","        return not write\n"]},{"cell_type":"code","execution_count":46,"id":"b3a8faf7","metadata":{"executionInfo":{"elapsed":219,"status":"ok","timestamp":1725921393196,"user":{"displayName":"Aditya Singhal","userId":"02233461186993627837"},"user_tz":240},"id":"b3a8faf7"},"outputs":[],"source":["# TypeOfTextFragmentEnum and TypeOfTextFragment\n","class TypeOfTextFragmentEnum(Enum):\n","    TERM = \"TERM\"\n","    NGRAM = \"NGRAM\"\n","    ONOMATOPAE = \"ONOMATOPAE\"\n","    TEXTO_SPEAK = \"TEXTO_SPEAK\"\n","    EMOTICON_IN_ASCII = \"EMOTICON_IN_ASCII\"\n","    WHITE_SPACE = \"WHITE_SPACE\"\n","    EMOJI = \"EMOJI\"\n","    PUNCTUATION = \"PUNCTUATION\"\n","    QUESTION = \"QUESTION\"\n","    TOO_SHORT = \"TOO_SHORT\"\n","    HASHTAG = \"HASHTAG\"\n","\n","class TypeOfTextFragment:\n","    def __init__(self, type_of_token_name: Optional[str] = None, type_of_token_enum: Optional[TypeOfTextFragmentEnum] = None):\n","        if type_of_token_enum:\n","            self.type_of_text_fragment_enum = type_of_token_enum\n","        elif type_of_token_name:\n","            self.set_type_of_text_fragment_name(type_of_token_name)\n","        else:\n","            self.type_of_text_fragment_enum = TypeOfTextFragmentEnum.NGRAM\n","\n","    def get_type_of_text_fragment_enum(self) -> TypeOfTextFragmentEnum:\n","        return self.type_of_text_fragment_enum\n","\n","    def set_type_of_text_fragment_name(self, type_of_token_name: str):\n","        try:\n","            self.type_of_text_fragment_enum = TypeOfTextFragmentEnum[type_of_token_name]\n","        except KeyError:\n","            print(f\"Error: type of token name '{type_of_token_name}' is not a valid name\")\n","            self.type_of_text_fragment_enum = TypeOfTextFragmentEnum.NGRAM\n"]},{"cell_type":"code","execution_count":47,"id":"429fe93b","metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1725921393196,"user":{"displayName":"Aditya Singhal","userId":"02233461186993627837"},"user_tz":240},"id":"429fe93b"},"outputs":[],"source":["# TextFragment\n","class TextFragment:\n","    def __init__(self):\n","        self.index_cardinal = 0\n","        self.index_ordinal = 0\n","        self.index_cardinal_in_sentence = 0\n","        self.index_ordinal_in_sentence = 0\n","        self.length = 0\n","        self.type_of_text_fragment_enum: Optional[TypeOfTextFragmentEnum] = None\n","        self.sentence_like_fragment_index = 0\n","\n","        self.original_form = \"\"\n","        self.original_form_lemmatized = \"\"\n","\n","    def get_original_form(self) -> str:\n","        return self.original_form\n","\n","    def set_original_form(self, original_form: str):\n","        self.original_form = original_form\n","\n","    def add_char_to_original_form(self, c: str):\n","        if len(c) == 1:  # Ensure it's a single character\n","            self.original_form += c\n","\n","    def add_string_to_original_form(self, s: str):\n","        self.original_form += s\n","\n","    def get_index_cardinal(self) -> int:\n","        return self.index_cardinal\n","\n","    def set_index_cardinal(self, index_cardinal: int):\n","        self.index_cardinal = index_cardinal\n","\n","    def get_index_ordinal(self) -> int:\n","        return self.index_ordinal\n","\n","    def set_index_ordinal(self, index_ordinal: int):\n","        self.index_ordinal = index_ordinal\n","\n","    def get_length(self) -> int:\n","        return self.length\n","\n","    def set_length(self, length: int):\n","        self.length = length\n","\n","    def get_type_of_text_fragment_enum(self) -> Optional[TypeOfTextFragmentEnum]:\n","        return self.type_of_text_fragment_enum\n","\n","    def get_index_cardinal_in_sentence(self) -> int:\n","        return self.index_cardinal_in_sentence\n","\n","    def set_index_cardinal_in_sentence(self, index_cardinal_in_sentence: int):\n","        self.index_cardinal_in_sentence = index_cardinal_in_sentence\n","\n","    def get_index_ordinal_in_sentence(self) -> int:\n","        return self.index_ordinal_in_sentence\n","\n","    def set_index_ordinal_in_sentence(self, index_ordinal_in_sentence: int):\n","        self.index_ordinal_in_sentence = index_ordinal_in_sentence\n","\n","    def get_original_form_lemmatized(self) -> str:\n","        return self.original_form_lemmatized\n","\n","    def set_original_form_lemmatized(self, original_form_lemmatized: str):\n","        self.original_form_lemmatized = original_form_lemmatized\n","\n","    def get_sentence_like_fragment_index(self) -> int:\n","        return self.sentence_like_fragment_index\n","\n","    def set_sentence_like_fragment_index(self, sentence_like_fragment_index: int):\n","        self.sentence_like_fragment_index = sentence_like_fragment_index\n","\n","    def __hash__(self) -> int:\n","        return hash(self.original_form)\n","\n","    def __eq__(self, other) -> bool:\n","        if isinstance(other, TextFragment):\n","            return self.original_form == other.original_form\n","        return False"]},{"cell_type":"code","execution_count":48,"id":"001ad168","metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1725921393196,"user":{"displayName":"Aditya Singhal","userId":"02233461186993627837"},"user_tz":240},"id":"001ad168"},"outputs":[],"source":["class Emoji(TextFragment):\n","    def __init__(self):\n","        super().__init__()\n","        self.semi_colon_form: Optional[str] = None\n","\n","    def get_semi_colon_form(self) -> Optional[str]:\n","        return self.semi_colon_form\n","\n","    def set_semi_colon_form(self, semi_colon_form: str):\n","        self.semi_colon_form = semi_colon_form\n","\n","    def get_type_of_text_fragment_enum(self) -> Optional['TypeOfTextFragmentEnum']:\n","        return TypeOfTextFragmentEnum.EMOJI\n"]},{"cell_type":"code","execution_count":49,"id":"6b1514a4","metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1725921393196,"user":{"displayName":"Aditya Singhal","userId":"02233461186993627837"},"user_tz":240},"id":"6b1514a4"},"outputs":[],"source":["class Category:\n","    class CategoryEnum(Enum):\n","        _10 = \"neutral tone\"\n","        _11 = \"positive tone\"\n","        _111 = \"positive tone, not promoted\"\n","        _12 = \"negative tone\"\n","        _13 = \"possibly ironic tone\"\n","        _14 = \"fun tone\"\n","        _17 = \"delight\"\n","        _20 = \"neutral intensity\"\n","        _21 = \"weak intensity\"\n","        _22 = \"strong intensity\"\n","        _3 = \"time\"\n","        _30 = \"neutral time\"\n","        _31 = \"past time\"\n","        _311 = \"immediate past\"\n","        _320 = \"present time\"\n","        _321 = \"immediate present: just now\"\n","        _33 = \"future time\"\n","        _331 = \"immediate future\"\n","        _40 = \"question\"\n","        _50 = \"neutral address\"\n","        _51 = \"subjective address\"\n","        _52 = \"direct address\"\n","        _521 = \"call to action\"\n","        _60 = \"neutral topic\"\n","        _61 = \"commercial tone / promoted\"\n","        _611 = \"commercial offer\"\n","        _612 = \"tweeted by the client\"\n","        _6121 = \"a retweet of the client's tweet\"\n","        _62 = \"factual statement\"\n","        _621 = \"factual statement - statistics cited\"\n","        _9 = \"not suitable for semantic analysis\"\n","        _91 = \"english text not detected\"\n","        _92 = \"text too short or garbled\"\n","\n","        def __str__(self):\n","            return self.value\n","\n","    def __init__(self, cat_number: str):\n","        self.category_enum: Category.CategoryEnum = self.set_category_enum_from_string(cat_number)\n","\n","    def get_category_enum(self) -> 'CategoryEnum':\n","        return self.category_enum\n","\n","    def set_category_enum(self, category_enum: 'CategoryEnum'):\n","        self.category_enum = category_enum\n","\n","    def set_category_enum_from_string(self, category_enum_from_string: str) -> 'CategoryEnum':\n","        is_valid_category_name = False\n","        for c in Category.CategoryEnum:\n","            if c.name == f\"_{category_enum_from_string}\":\n","                is_valid_category_name = True\n","                return c\n","\n","        if not is_valid_category_name:\n","            print(\"error in class Category\")\n","            print(f\"category name {category_enum_from_string} is not a valid name\")\n","            return Category.CategoryEnum._10\n"]},{"cell_type":"code","execution_count":50,"id":"323e99e6","metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1725921393197,"user":{"displayName":"Aditya Singhal","userId":"02233461186993627837"},"user_tz":240},"id":"323e99e6"},"outputs":[],"source":["# Assuming that TypeOfTextFragmentEnum and Category classes are defined elsewhere\n","class PatternOfInterest:\n","    def __init__(self):\n","        self.description: str = \"\"\n","        self.regex: str = \"\"\n","        self.should_apply_to_lower_case_text: bool = False\n","        self.categories: List['Category'] = []\n","        self.type_of_text_fragment_enum: Optional['TypeOfTextFragmentEnum'] = None\n","        self.pattern: Optional[re.Pattern] = None\n","        self.matched: Optional[bool] = None\n","\n","    def get_description(self) -> str:\n","        return self.description\n","\n","    def set_description(self, description: str):\n","        self.description = description\n","\n","    def get_regex(self) -> str:\n","        return self.regex\n","\n","    def set_regex(self, regex: str):\n","        self.regex = regex\n","        self.pattern = re.compile(regex)\n","\n","    def is_should_apply_to_lower_case_text(self) -> bool:\n","        return self.should_apply_to_lower_case_text\n","\n","    def set_should_apply_to_lower_case_text(self, should_apply_to_lower_case_text: bool):\n","        self.should_apply_to_lower_case_text = should_apply_to_lower_case_text\n","\n","    def get_categories(self) -> List['Category']:\n","        return self.categories\n","\n","    def set_categories(self, categories: List['Category']):\n","        self.categories = categories\n","\n","    def get_type_of_text_fragment_enum(self) -> Optional['TypeOfTextFragmentEnum']:\n","        return self.type_of_text_fragment_enum\n","\n","    def set_type_of_text_fragment(self, type_of_token_name: str):\n","        # Assuming TypeOfTextFragment is another class with a method get_type_of_text_fragment_enum\n","        self.type_of_text_fragment_enum = TypeOfTextFragment(type_of_token_name).get_type_of_text_fragment_enum()\n","\n","    def set_type_of_text_fragment_enum(self, type_of_text_fragment_enum: 'TypeOfTextFragmentEnum'):\n","        self.type_of_text_fragment_enum = type_of_text_fragment_enum\n","\n","    def get_pattern(self) -> Optional[re.Pattern]:\n","        return self.pattern\n","\n","    def set_pattern(self, pattern: re.Pattern):\n","        self.pattern = pattern\n","\n","    def get_matched(self) -> Optional[bool]:\n","        return self.matched\n","\n","    def set_matched(self, matched: bool):\n","        self.matched = matched\n"]},{"cell_type":"code","execution_count":51,"id":"65698eb7","metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1725921393197,"user":{"displayName":"Aditya Singhal","userId":"02233461186993627837"},"user_tz":240},"id":"65698eb7"},"outputs":[],"source":["# Assuming that TypeOfTextFragmentEnum and PatternOfInterest classes are defined elsewhere\n","class NonWord(TextFragment):\n","    def __init__(self):\n","        super().__init__()\n","        self.poi: Optional[PatternOfInterest] = None\n","        self.type_of_text_fragment_enum: Optional[TypeOfTextFragmentEnum] = None\n","\n","    def get_type_of_text_fragment_enum(self) -> Optional[TypeOfTextFragmentEnum]:\n","        return self.type_of_text_fragment_enum\n","\n","    def get_poi(self) -> Optional[PatternOfInterest]:\n","        return self.poi\n","\n","    def set_poi(self, poi: 'PatternOfInterest'):\n","        self.poi = poi\n","        self.type_of_text_fragment_enum = poi.get_type_of_text_fragment_enum()\n","\n","    def set_type_of_text_fragment_enum(self, type_of_text_fragment_enum: 'TypeOfTextFragmentEnum'):\n","        self.type_of_text_fragment_enum = type_of_text_fragment_enum\n"]},{"cell_type":"code","execution_count":52,"id":"4bba6ec8","metadata":{"executionInfo":{"elapsed":163,"status":"ok","timestamp":1725921393352,"user":{"displayName":"Aditya Singhal","userId":"02233461186993627837"},"user_tz":240},"id":"4bba6ec8"},"outputs":[],"source":["class Punctuation(TextFragment):\n","    def get_type_of_text_fragment_enum(self) -> 'TypeOfTextFragmentEnum':\n","        return TypeOfTextFragmentEnum.PUNCTUATION\n","\n","    def to_non_word(self, poi: 'PatternOfInterest', string: str) -> 'NonWord':\n","        non_word = NonWord()\n","        non_word.set_poi(poi)\n","        non_word.set_original_form(string)\n","        non_word.set_type_of_text_fragment_enum(poi.get_type_of_text_fragment_enum())\n","        non_word.set_index_cardinal(self.get_index_cardinal())\n","        non_word.set_index_ordinal(self.get_index_ordinal())\n","        non_word.set_type_of_text_fragment_enum(poi.get_type_of_text_fragment_enum())\n","\n","        return non_word\n"]},{"cell_type":"code","execution_count":53,"id":"10cc1710","metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1725921393352,"user":{"displayName":"Aditya Singhal","userId":"02233461186993627837"},"user_tz":240},"id":"10cc1710"},"outputs":[],"source":["class WhiteSpace(TextFragment):\n","    def __init__(self):\n","        super().__init__()\n","        self.sentence_or_line_break = False\n","\n","    def get_type_of_text_fragment_enum(self) -> 'TypeOfTextFragmentEnum':\n","        return TypeOfTextFragmentEnum.WHITE_SPACE\n","\n","    def is_sentence_or_line_break(self) -> bool:\n","        return self.sentence_or_line_break\n","\n","    def set_sentence_or_line_break(self, sentence_or_line_break: bool):\n","        self.sentence_or_line_break = sentence_or_line_break\n"]},{"cell_type":"code","execution_count":54,"id":"e3ee60d2","metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1725921393352,"user":{"displayName":"Aditya Singhal","userId":"02233461186993627837"},"user_tz":240},"id":"e3ee60d2"},"outputs":[],"source":["# Term\n","class Term(TextFragment):\n","    def __init__(self):\n","        super().__init__()\n","        self.cleaned_form: Optional[str] = None\n","        self.cleaned_and_stripped_form: Optional[str] = None\n","\n","    def get_type_of_text_fragment_enum(self) -> TypeOfTextFragmentEnum:\n","        return TypeOfTextFragmentEnum.TERM\n","\n","    @property\n","    def cleaned_form(self) -> Optional[str]:\n","        return self._cleaned_form\n","\n","    @cleaned_form.setter\n","    def cleaned_form(self, value: str):\n","        self._cleaned_form = value\n","\n","    @property\n","    def cleaned_and_stripped_form(self) -> Optional[str]:\n","        return self._cleaned_and_stripped_form\n","\n","    @cleaned_and_stripped_form.setter\n","    def cleaned_and_stripped_form(self, value: str):\n","        self._cleaned_and_stripped_form = value\n","\n","    def get_cleaned_and_stripped_if_condition(self, stripped: bool) -> Optional[str]:\n","        return self.cleaned_and_stripped_form if stripped else self.cleaned_form\n","\n","    def to_ngram(self) -> 'NGram':\n","        ngram = NGram()\n","        ngram.set_index_cardinal(self.get_index_cardinal())\n","        ngram.set_index_cardinal_in_sentence(self.get_index_cardinal_in_sentence())\n","        ngram.set_index_ordinal(self.get_index_ordinal())\n","        ngram.set_index_ordinal_in_sentence(self.get_index_ordinal_in_sentence())\n","        ngram.set_original_form(self.get_original_form())\n","        ngram.terms.append(self)\n","        return ngram"]},{"cell_type":"code","execution_count":55,"id":"ce5066ae","metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1725921393352,"user":{"displayName":"Aditya Singhal","userId":"02233461186993627837"},"user_tz":240},"id":"ce5066ae"},"outputs":[],"source":["# Ngram\n","from typing import List\n","\n","class NGram(TextFragment):\n","    def __init__(self):\n","        super().__init__()\n","        self.terms: List[Term] = []\n","\n","    def get_terms(self) -> List[Term]:\n","        return self.terms\n","\n","    def set_terms(self, terms: List[Term]):\n","        self.terms = terms\n","\n","    def get_type_of_text_fragment_enum(self) -> TypeOfTextFragmentEnum:\n","        return TypeOfTextFragmentEnum.NGRAM\n","\n","    def get_cleaned_and_stripped_ngram(self) -> str:\n","        return ' '.join(term.cleaned_and_stripped_form for term in self.terms).strip()\n","\n","    def get_cleaned_and_stripped_ngram_if_condition(self, stripped: bool) -> str:\n","        return ' '.join(term.get_cleaned_and_stripped_if_condition(stripped) for term in self.terms).strip()\n","\n","    def get_cleaned_ngram(self) -> str:\n","        return ' '.join(term.cleaned_form for term in self.terms).strip()\n"]},{"cell_type":"code","execution_count":56,"id":"44e49d55","metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1725921393352,"user":{"displayName":"Aditya Singhal","userId":"02233461186993627837"},"user_tz":240},"id":"44e49d55"},"outputs":[],"source":["class SentenceLike:\n","    def __init__(self):\n","        self.ngrams: List[NGram] = []\n","        self.text_fragments: List[TextFragment] = []\n","        self.index_ordinal: int = 0\n","        self.index_cardinal: int = 0\n","\n","    def get_ngrams(self) -> List[NGram]:\n","        return self.ngrams\n","\n","    def set_ngrams(self, ngrams: List[NGram]):\n","        self.ngrams = ngrams\n","\n","    def get_text_fragments(self) -> List[TextFragment]:\n","        return self.text_fragments\n","\n","    def set_text_fragments(self, text_fragments: List[TextFragment]):\n","        self.text_fragments = text_fragments\n","\n","    def get_index_ordinal(self) -> int:\n","        return self.index_ordinal\n","\n","    def set_index_ordinal(self, index_ordinal: int):\n","        self.index_ordinal = index_ordinal\n","\n","    def get_index_cardinal(self) -> int:\n","        return self.index_cardinal\n","\n","    def set_index_cardinal(self, index_cardinal: int):\n","        self.index_cardinal = index_cardinal\n","\n","    def __str__(self) -> str:\n","        return ''.join(tf.get_original_form() for tf in self.text_fragments)\n"]},{"cell_type":"code","execution_count":57,"id":"cabea579","metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1725921393352,"user":{"displayName":"Aditya Singhal","userId":"02233461186993627837"},"user_tz":240},"id":"cabea579"},"outputs":[],"source":["class NGramFinderBisForTextFragments:\n","    @staticmethod\n","    def generate_ngrams_upto(ngrams: List[NGram], max_gram_size: int) -> List[NGram]:\n","        text_fragments_augmented_with_ngrams = []\n","        ngram_size = 0\n","        it = deque(ngrams)  # Use deque for efficient indexing\n","        length = len(it)\n","\n","        for i in range(length):\n","            unigram = it[i]\n","\n","            if not isinstance(unigram, NGram):\n","                print(\"Alert: a non-NGram detected in method generate_ngrams_upto\")\n","                print(\"TextFragment was:\", unigram.get_original_form())\n","                continue\n","\n","            unigram.set_index_ordinal_in_sentence(i)\n","\n","            # 1. Add the word itself\n","            text_fragments_augmented_with_ngrams.append(unigram)\n","\n","            # 2. Open a new NGram\n","            ngram = NGram()\n","            ngram.set_terms([unigram.get_terms()[0]])\n","            ngram_size = 1\n","\n","            # 2- Insert previous terms of the word and add those too\n","            for j in range(i - 1, -1, -1):\n","                if ngram_size >= max_gram_size:\n","                    break\n","\n","                previous_unigram = it[j]\n","                previous_term = previous_unigram.get_terms()[0]\n","                ngram.get_terms().insert(0, previous_term)\n","                new_ngram = NGram()\n","                new_ngram.set_terms(list(ngram.get_terms()))\n","                new_ngram.set_index_cardinal(previous_unigram.index_cardinal)\n","                new_ngram.set_index_ordinal(previous_unigram.index_ordinal)\n","                new_ngram.set_index_cardinal_in_sentence(previous_unigram.index_cardinal_in_sentence)\n","                new_ngram.set_index_ordinal_in_sentence(previous_unigram.index_ordinal_in_sentence)\n","                new_ngram.set_original_form(new_ngram.get_cleaned_and_stripped_ngram_if_condition(stripped=True))\n","                text_fragments_augmented_with_ngrams.append(new_ngram)\n","                ngram_size += 1\n","\n","        return text_fragments_augmented_with_ngrams"]},{"cell_type":"code","execution_count":58,"id":"dd4895a8","metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1725921393352,"user":{"displayName":"Aditya Singhal","userId":"02233461186993627837"},"user_tz":240},"id":"dd4895a8"},"outputs":[],"source":["class SentenceLikeFragmentsDetector:\n","    def __init__(self):\n","        self.stop_punctuations: Set[str] = {\".\", \":\", \";\", \",\", \"(\", \")\", \"\\\"\", \"«\", \"»\", \"“\", \"”\", \"•\", \"‘\", \"’\", \"'\", \"„\", \"[\", \"]\", \"\", \"<\", \">\"}\n","        self.matching_punctuations: Dict[str, str] = {\n","            \"(\": \")\", \"\\\"\": \"\\\"\", \"«\": \"»\", \"“\": \"”\", \"‘\": \"’\", \"'\": \"'\", \"„\": \"“\", \"[\": \"]\", \"<\": \">\"\n","        }\n","        self.list_of_sentence_like_fragments: List[SentenceLike] = []\n","        self.list_of_n_grams: List[NGram] = []\n","        self.sentence_like = SentenceLike()\n","        self.text_fragment_already_added_to_a_sentence = False\n","        self.opening_punctuation_identified = False\n","        self.expected_closing_punctuation_sign = \"\"\n","        self.sentence_like_fragments_counter = 0\n","\n","    def return_sentence_like_fragments(self, text_fragments: List[TextFragment]) -> List[SentenceLike]:\n","        self.sentence_like.set_index_cardinal(0)\n","        self.sentence_like.set_index_ordinal(0)\n","\n","        for next_text_fragment in text_fragments:\n","            self.text_fragment_already_added_to_a_sentence = False\n","            type_of_text_fragment = next_text_fragment.get_type_of_text_fragment_enum()\n","\n","            if type_of_text_fragment == TypeOfTextFragmentEnum.TERM:\n","                self.add_term_to_n_grams_of_current_sentence(next_text_fragment)\n","            elif type_of_text_fragment == TypeOfTextFragmentEnum.PUNCTUATION:\n","                punctuation_sign = next_text_fragment.get_original_form()\n","                if punctuation_sign in self.matching_punctuations and not self.opening_punctuation_identified:\n","                    self.opening_punctuation_identified = True\n","                    self.expected_closing_punctuation_sign = self.matching_punctuations[punctuation_sign]\n","                    self.close_current_sentence_and_open_new_one(False)\n","                    self.add_text_fragment_to_current_sentence(next_text_fragment)\n","                elif punctuation_sign in self.stop_punctuations:\n","                    if self.opening_punctuation_identified:\n","                        self.add_text_fragment_to_current_sentence(next_text_fragment)\n","                        if punctuation_sign == self.expected_closing_punctuation_sign:\n","                            self.close_current_sentence_and_open_new_one(True)\n","                    else:\n","                        self.close_current_sentence_and_open_new_one(False)\n","\n","            self.add_text_fragment_to_current_sentence(next_text_fragment)\n","\n","        self.close_current_sentence_and_open_new_one(False)\n","        return self.list_of_sentence_like_fragments\n","\n","    def close_current_sentence_and_open_new_one(self, is_end_of_matching_signs: bool):\n","        self.sentence_like.get_ngrams().extend(self.list_of_n_grams)\n","        if self.sentence_like.get_text_fragments():\n","            self.list_of_sentence_like_fragments.append(self.sentence_like)\n","        self.sentence_like_fragments_counter += 1\n","        self.sentence_like = SentenceLike()\n","        self.sentence_like.set_index_ordinal(len(self.list_of_sentence_like_fragments))\n","        self.list_of_n_grams = []\n","        if is_end_of_matching_signs:\n","            self.opening_punctuation_identified = False\n","            self.expected_closing_punctuation_sign = \"\"\n","\n","    def add_text_fragment_to_current_sentence(self, tf: TextFragment):\n","        if self.text_fragment_already_added_to_a_sentence:\n","            return\n","        if not self.sentence_like.get_text_fragments():\n","            self.sentence_like.set_index_cardinal(tf.get_index_cardinal())\n","        tf.set_index_ordinal_in_sentence(len(self.sentence_like.get_text_fragments()))\n","        tf.set_sentence_like_fragment_index(self.sentence_like_fragments_counter)\n","        self.sentence_like.get_text_fragments().append(tf)\n","        self.text_fragment_already_added_to_a_sentence = True\n","\n","    def add_term_to_n_grams_of_current_sentence(self, next_text_fragment: TextFragment):\n","        term = next_text_fragment\n","        ngram = NGram()\n","        ngram.set_index_cardinal(term.get_index_cardinal())\n","        ngram.set_index_ordinal(term.get_index_ordinal())\n","        ngram.set_index_ordinal_in_sentence(term.get_index_ordinal_in_sentence())\n","        ngram.set_sentence_like_fragment_index(self.sentence_like_fragments_counter)\n","        ngram.get_terms().append(term)\n","        ngram.set_original_form(term.get_original_form())\n","        self.list_of_n_grams.append(ngram)\n"]},{"cell_type":"code","execution_count":59,"id":"51978d24","metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1725921393352,"user":{"displayName":"Aditya Singhal","userId":"02233461186993627837"},"user_tz":240},"id":"51978d24"},"outputs":[],"source":["class Clock:\n","    def __init__(self, action_being_clocked, start_silent=False):\n","        \"\"\"\n","        Initializes the Clock with the action to be clocked and optional silent mode.\n","        :param action_being_clocked: Description of the action being clocked.\n","        :param start_silent: Whether to start silently (default is False).\n","        \"\"\"\n","        self.action_being_clocked = action_being_clocked\n","        self.silent = start_silent\n","        self.start = time.time()\n","        self.log_text = []\n","        self.intermediate_text = []\n","\n","        if not self.silent:\n","            self.start_clock()\n","\n","    def start_clock(self):\n","        \"\"\"\n","        Starts the clock and prints the start message if not in silent mode.\n","        \"\"\"\n","        self.start = time.time()\n","        self.log_text.append(f\"{self.action_being_clocked}...\")\n","        if not self.silent:\n","            print(self.log_text[-1])\n","\n","    def start_clock_to_string(self):\n","        \"\"\"\n","        Starts the clock and returns the start message as a string.\n","        :return: Start message string.\n","        \"\"\"\n","        self.start = time.time()\n","        message = f\"{self.action_being_clocked}...\"\n","        return message\n","\n","    def print_intermediary_text(self, intermediary_text):\n","        \"\"\"\n","        Appends intermediary text to the log and prints it if not in silent mode.\n","        :param intermediary_text: The text to append and print.\n","        \"\"\"\n","        self.intermediate_text.append(intermediary_text)\n","        if not self.silent:\n","            print('\\n'.join(self.intermediate_text))\n","        self.intermediate_text = []\n","\n","    def print_intermediary_text_to_string(self, it):\n","        \"\"\"\n","        Appends intermediary text and returns it as a string.\n","        :param it: The text to append.\n","        :return: Intermediary text string.\n","        \"\"\"\n","        self.intermediate_text = [it]\n","        return '\\n'.join(self.intermediate_text)\n","\n","    def get_elapsed_time(self):\n","        \"\"\"\n","        Returns the elapsed time in milliseconds since the clock was started.\n","        :return: Elapsed time in milliseconds.\n","        \"\"\"\n","        current_time = time.time()\n","        return int((current_time - self.start) * 1000)\n","\n","    def print_elapsed_time(self):\n","        \"\"\"\n","        Prints the elapsed time if not in silent mode.\n","        \"\"\"\n","        if not self.silent:\n","            print(self.compute_elapsed_time())\n","\n","    def print_elapsed_time_to_string(self):\n","        \"\"\"\n","        Returns the elapsed time as a string.\n","        :return: Elapsed time string.\n","        \"\"\"\n","        return self.compute_elapsed_time()\n","\n","    def compute_elapsed_time(self):\n","        \"\"\"\n","        Computes the elapsed time in a human-readable format.\n","        :return: Elapsed time string.\n","        \"\"\"\n","        elapsed_time = time.time() - self.start\n","        if elapsed_time < 10:\n","            seconds = round(elapsed_time)\n","            return f\"{seconds} seconds\"\n","        elif elapsed_time < 60:\n","            return f\"{int(elapsed_time)} seconds\"\n","        elif elapsed_time < 3600:\n","            minutes = int(elapsed_time // 60)\n","            seconds = int(elapsed_time % 60)\n","            return f\"{minutes} minutes {seconds} seconds\"\n","        else:\n","            hours = int(elapsed_time // 3600)\n","            minutes = int((elapsed_time % 3600) // 60)\n","            seconds = int(elapsed_time % 60)\n","            return f\"{hours} hours {minutes} minutes {seconds} seconds\"\n","\n","    def close_and_print_clock(self, closing_message=\"\"):\n","        \"\"\"\n","        Ends the clock, prints the closing message and duration if not in silent mode.\n","        :param closing_message: Optional closing message.\n","        \"\"\"\n","        if not self.silent:\n","            print(self.write_log_text_closing(closing_message))\n","\n","    def close_and_print_clock_to_string(self, closing_message=\"\"):\n","        \"\"\"\n","        Ends the clock and returns the closing message and duration as a string.\n","        :param closing_message: Optional closing message.\n","        :return: Closing message string.\n","        \"\"\"\n","        return self.write_log_text_closing(closing_message)\n","\n","    def write_log_text_closing(self, closing_message):\n","        \"\"\"\n","        Prepares the closing log text with the action description and elapsed time.\n","        :param closing_message: Optional closing message.\n","        :return: Closing log text.\n","        \"\"\"\n","        self.log_text = []\n","        closing_log = (\n","            f\"{closing_message}\\n\"\n","            f\"finished {self.action_being_clocked}. [Duration: {self.compute_elapsed_time()}]\"\n","        )\n","        return closing_log\n","\n","    def get_action(self):\n","        \"\"\"\n","        Returns the description of the action being clocked.\n","        :return: Action description.\n","        \"\"\"\n","        return self.action_being_clocked\n"]},{"cell_type":"code","execution_count":60,"id":"98f50bef","metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1725921393352,"user":{"displayName":"Aditya Singhal","userId":"02233461186993627837"},"user_tz":240},"id":"98f50bef"},"outputs":[],"source":["class Multiset:\n","    def __init__(self, max_elements=None):\n","        self.internal_map = defaultdict(int)\n","        self.max_elements = max_elements\n","\n","    def get_internal_map(self):\n","        return dict(self.internal_map)\n","\n","    def set_internal_map(self, internal_map):\n","        self.internal_map = defaultdict(int, internal_map)\n","\n","    def set_count(self, element, count):\n","        self.internal_map[element] = count\n","\n","    def add_one(self, element):\n","        self.internal_map[element] += 1\n","\n","    def add_one_with_limit_to_max_elements(self, element):\n","        if self.max_elements is not None and len(self.internal_map) >= self.max_elements:\n","            return\n","        self.internal_map[element] += 1\n","\n","    def add_several(self, element, count):\n","        self.internal_map[element] += count\n","\n","    def add_all_from_multiset(self, other_multiset):\n","        for element, count in other_multiset.get_entry_set():\n","            self.add_several(element, count)\n","\n","    def add_all_from_map(self, map):\n","        for element, count in map.items():\n","            self.add_several(element, count)\n","\n","    def add_all_from_list_or_set(self, collection):\n","        for element in collection:\n","            self.add_one(element)\n","\n","    def remove_one(self, element):\n","        if element in self.internal_map:\n","            if self.internal_map[element] > 1:\n","                self.internal_map[element] -= 1\n","            else:\n","                del self.internal_map[element]\n","\n","    def remove_several(self, element, number_to_be_removed):\n","        if element in self.internal_map:\n","            if self.internal_map[element] > number_to_be_removed:\n","                self.internal_map[element] -= number_to_be_removed\n","            else:\n","                del self.internal_map[element]\n","\n","    def get_count(self, element):\n","        return self.internal_map.get(element, 0)\n","\n","    def get_size(self):\n","        return len(self.internal_map)\n","\n","    def get_element_set(self):\n","        return set(self.internal_map.keys())\n","\n","    def get_entry_set(self):\n","        return self.internal_map.items()\n","\n","    def sort_by_freq(self):\n","        return OrderedDict(sorted(self.internal_map.items(), key=lambda item: item[1]))\n","\n","    def sort_desc(self, multiset):\n","        return sorted(multiset.get_entry_set(), key=lambda item: item[1], reverse=True)\n","\n","    def sort_asc(self, multiset):\n","        return sorted(multiset.get_entry_set(), key=lambda item: item[1])\n","\n","    def sort_desc_keep_most_frequent(self, multiset, n):\n","        return list(islice(self.sort_desc(multiset), n))\n","\n","    def keep_most_frequent(self, multiset, n):\n","        new_multiset = Multiset()\n","        for element, count in self.sort_desc_keep_most_frequent(multiset, n):\n","            new_multiset.add_several(element, count)\n","        return new_multiset\n","\n","    def sort_desc_keep_above_min_freq(self, multiset, n):\n","        return [entry for entry in self.sort_desc(multiset) if entry[1] > n]\n","\n","    def to_list_of_elements(self):\n","        return list(self.internal_map.keys())\n","\n","    def to_list_of_all_occurrences(self):\n","        return [element for element, count in self.internal_map.items() for _ in range(count)]\n","\n","    def print_top_ranked_elements(self, top_rank):\n","        top_elements = self.sort_desc_keep_most_frequent(self, top_rank)\n","        for element, count in top_elements:\n","            print(f\"{element} x {count}\")\n","\n","    def top_ranked_elements_to_string(self, top_rank):\n","        top_elements = self.sort_desc_keep_most_frequent(self, top_rank)\n","        return \", \".join([f\"{element} x {count}\" for element, count in top_elements])\n","\n","    def top_ranked_elements_to_string_without_counts(self, top_rank):\n","        top_elements = self.sort_desc_keep_most_frequent(self, top_rank)\n","        return \", \".join([str(element) for element, _ in top_elements])\n"]},{"cell_type":"code","execution_count":61,"id":"4ff47363","metadata":{"executionInfo":{"elapsed":183,"status":"ok","timestamp":1725921393532,"user":{"displayName":"Aditya Singhal","userId":"02233461186993627837"},"user_tz":240},"id":"4ff47363"},"outputs":[],"source":["class NGramDuplicatesCleaner:\n","    def __init__(self, stop_words: Set[str] = None):\n","        if stop_words is None:\n","            stop_words = set()\n","        self.stop_words = stop_words\n","        self.multiset_words = Multiset()\n","        self.words_to_be_removed = set()\n","\n","    def remove_duplicates(self, map_ngrams: Dict[str, int], max_grams: int, remove_single_terms: bool) -> Dict[str, int]:\n","        # Set the factor for removing irrelevant unigrams based on the number of n-grams\n","        if len(map_ngrams) < 500:\n","            factor_removing_irrelevant_unigrams = 2.0\n","            min_occurrences = 2\n","        else:\n","            factor_removing_irrelevant_unigrams = 1.5\n","            min_occurrences = 3 if len(map_ngrams) > 10_000 else 2\n","\n","        # Remove terms that appear just once in the corpus\n","        map_ngrams = {k: v for k, v in map_ngrams.items() if v >= min_occurrences}\n","\n","        # Iterate from max_grams to 1 to remove less frequent terms\n","        for i in range(max_grams - 1, 0, -1):\n","            it_freq_list = iter(map_ngrams.items())\n","            for entry in it_freq_list:\n","                curr_word = entry[0].strip()\n","                count_in = curr_word.count(' ')\n","\n","                if count_in == i:\n","                    if i == 1:\n","                        terms_in_bigram = curr_word.split(\" \")\n","                        term1, term2 = terms_in_bigram[0].strip(), terms_in_bigram[1].strip()\n","\n","                        if term1 in self.stop_words and term2 in self.stop_words:\n","                            self.words_to_be_removed.add(curr_word)\n","                        if term1 in self.stop_words:\n","                            self.words_to_be_removed.add(term1)\n","                        if term2 in self.stop_words:\n","                            self.words_to_be_removed.add(term2)\n","\n","                        count_term1 = map_ngrams.get(term1)\n","                        count_term2 = map_ngrams.get(term2)\n","\n","                        if count_term1 is not None and count_term1 < entry[1] * factor_removing_irrelevant_unigrams:\n","                            self.words_to_be_removed.add(term1)\n","                        if count_term2 is not None and count_term2 < entry[1] * factor_removing_irrelevant_unigrams:\n","                            self.words_to_be_removed.add(term2)\n","                    else:\n","                        set_current_sub_ngrams = NGramFinder.ngrams_finder_just_a_given_length(i, curr_word).get_element_set()\n","                        for inner_ngram in set_current_sub_ngrams:\n","                            inner_ngram = inner_ngram.strip()\n","                            if inner_ngram in map_ngrams:\n","                                if map_ngrams[inner_ngram] < entry[1] * factor_removing_irrelevant_unigrams:\n","                                    first_term_outer_ngram = curr_word.split(\" \")[0]\n","                                    if first_term_outer_ngram not in self.stop_words:\n","                                        self.words_to_be_removed.add(inner_ngram)\n","\n","        # Add the terms that should remain to the multiset_words\n","        for curr_word, count in map_ngrams.items():\n","            if curr_word not in self.words_to_be_removed or not self.stop_words:\n","                self.multiset_words.add_several(curr_word.strip(), count)\n","\n","        return self.multiset_words.get_internal_map()\n","\n"]},{"cell_type":"code","execution_count":62,"id":"4d1fc3a7","metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1725921393533,"user":{"displayName":"Aditya Singhal","userId":"02233461186993627837"},"user_tz":240},"id":"4d1fc3a7"},"outputs":[],"source":["class RepeatedCharactersRemover:\n","\n","    @staticmethod\n","    def repeated_characters(curr_term: str, terms_that_should_not_be_modified: Set[str]) -> str:\n","        to_return = curr_term\n","        index = None\n","        set_rl = set()\n","        count = 1\n","        chars = list(curr_term)\n","        curr_char = ''\n","        previous_char = ''\n","\n","        for i in range(len(chars)):\n","            curr_char = chars[i]\n","            if i > 0:\n","                previous_char = chars[i - 1]\n","            if previous_char == curr_char and RepeatedCharactersRemover.is_alphanumeric(previous_char):\n","                if index is None:\n","                    index = i - 1\n","                count += 1\n","            else:\n","                if count > 1:\n","                    set_rl.add((previous_char, index, count))\n","                    count = 1\n","                index = None\n","            if i == len(chars) - 1 and count > 1:\n","                set_rl.add((previous_char, index, count))\n","\n","        for prev_char, idx, cnt in set_rl:\n","            letter = prev_char\n","            to_replace = letter * cnt\n","            if cnt > 2:\n","                replace_with = letter * 2\n","                subs = to_return.replace(to_replace, replace_with)\n","                if subs.lower() in terms_that_should_not_be_modified:\n","                    to_return = subs\n","                else:\n","                    replace_with = letter\n","                    subs = to_return.replace(to_replace, replace_with)\n","                    if subs.lower() in terms_that_should_not_be_modified:\n","                        to_return = subs\n","\n","        return to_return\n","\n","    @staticmethod\n","    def is_alphanumeric(s: str) -> bool:\n","        return s.isalnum()"]},{"cell_type":"code","execution_count":63,"id":"96f9aa43","metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1725921393533,"user":{"displayName":"Aditya Singhal","userId":"02233461186993627837"},"user_tz":240},"id":"96f9aa43"},"outputs":[],"source":["class TextCleaningOps:\n","\n","    @staticmethod\n","    def clean(status):\n","        if status is None:\n","            return \"\"\n","        status = status.replace(\"...\", \" \")\n","        status = status.replace(\",\", \" \")\n","        status = status.replace(\"..\", \" \")\n","        status = re.sub(r'http[^ ]*', ' ', status)\n","        status = re.sub(r'http.*[\\r|\\n]*', ' ', status)\n","        status = re.sub(r' +', ' ', status)\n","        return status\n","\n","    @staticmethod\n","    def remove_punctuation_signs(string):\n","        if string is None:\n","            return \"\"\n","        punctuation = \"!?.@'’`+<>\\\"«»:-“”—+,|$;_/~&()[]{}#=*\"\n","        for char in punctuation:\n","            string = string.replace(char, \" \")\n","        return string.strip()\n","\n","    @staticmethod\n","    def detach_camel_case_words_and_put_in_lower_case(string):\n","        if \"LeMonde\" in string or \"PhD\" in string:\n","            return string\n","        result = []\n","        for i, char in enumerate(string):\n","            if i > 0 and char.isupper() and not string[i - 1].isupper():\n","                result.append(' ')\n","            result.append(char)\n","        return ''.join(result)\n","\n","    @staticmethod\n","    def remove_small_words(terms, less_or_equal_to_number):\n","        return {term: count for term, count in terms.items() if len(term) >= less_or_equal_to_number}\n","\n","    @staticmethod\n","    def should_it_be_removed(string, less_or_equal_to_number):\n","        return len(string.strip()) < less_or_equal_to_number or bool(re.search(r'\\d', string))\n","\n","    @staticmethod\n","    def remove_urls(status):\n","        status = re.sub(r'http[^ ]*', ' ', status)\n","        status = re.sub(r'http.*[\\r|\\n]*', ' ', status)\n","        status = re.sub(r' +', ' ', status)\n","        return status\n","\n","    @staticmethod\n","    def remove_start_and_final_apostrophs(string):\n","        string = string.replace(\"’\", \"'\")\n","        if string.endswith(\"'s\"):\n","            string = string[:-2]\n","        replacements = [\"l'\", \"d'\", \"m'\", \"t'\", \"j'\", \"c'\", \"n'\", \"s'\"]\n","        for rep in replacements:\n","            string = string.replace(rep, \" \")\n","        return string.strip()\n","\n","    @staticmethod\n","    def normalize_apostrophs(string):\n","        return string.replace(\"’\", \"'\")\n","\n","    @staticmethod\n","    def remove_terms_between_quotes(string):\n","        string = re.sub(r'\"[^\"]*\"', ' ', string)\n","        string = re.sub(r'«[^»]*»', ' ', string)\n","        string = re.sub(r'“[^”]*”', ' ', string)\n","        return string.strip()\n","\n","    @staticmethod\n","    def is_it_cleaned(status):\n","        return '\"' not in status or status.count('\"') % 2 == 0\n","\n","    @staticmethod\n","    def remove_small_words_or_numeric(terms, max_letters):\n","        return {term: count for term, count in terms.items() if len(term) >= max_letters and not bool(re.search(r'\\d', term))}\n","\n","    @staticmethod\n","    def remove_numeric(string):\n","        return re.sub(r'\\d', '', string)\n","\n","    @staticmethod\n","    def remove_hashtags(status):\n","        return re.sub(r'#\\p{L}+', '', status)\n","\n","    @staticmethod\n","    def put_in_lower_case(input_string):\n","        return input_string.lower()\n","\n","    @staticmethod\n","    def put_in_lower_case_map(map_of_lines):\n","        if map_of_lines is None:\n","            return {}\n","        return {k: (v.lower() if v is not None and not v.strip() == '' else '') for k, v in map_of_lines.items()}\n","\n","    @staticmethod\n","    def remove_xml_escaped(input_string):\n","        if input_string is None:\n","            return input_string\n","        replacements = {\n","            \"&gt;\": \" \",\n","            \"&lt;\": \" \",\n","            \"&amp;\": \" \",\n","            \"&apos;\": \" \",\n","            \"&quot;\": \" \"\n","        }\n","        for old, new in replacements.items():\n","            input_string = input_string.replace(old, new)\n","        return input_string\n","\n","    @staticmethod\n","    def remove_emojis_between_semi_colons(cleaned):\n","        pattern = re.compile(r':(.*?):')\n","        return pattern.sub(lambda m: ' ', cleaned)\n","\n","    @staticmethod\n","    def remove_null_chars(string):\n","        return string.replace('\\0', '').strip()\n","\n","    @staticmethod\n","    def flatten_to_ascii(string):\n","        if not string or not string.strip():\n","            return string\n","        normalized = unicodedata.normalize('NFD', string)\n","        ascii_only = ''.join(c for c in normalized if ord(c) <= 0x7F)\n","        return ascii_only\n","\n","    @staticmethod\n","    def flatten_to_ascii_and_remove_apostrophs(string):\n","        if not string or not string.strip():\n","            return string\n","        normalized = unicodedata.normalize('NFD', string)\n","        ascii_only = ''.join(c for c in normalized if ord(c) <= 0x7F and c not in (\"'\", \"’\"))\n","        return ascii_only\n","\n","    @staticmethod\n","    def do_all_cleaning_ops(map_of_lines):\n","        cleaned_lines = {}\n","        if map_of_lines is None:\n","            return cleaned_lines\n","        for k, status in map_of_lines.items():\n","            if status is None or not status.strip():\n","                cleaned_lines[k] = \"\"\n","                continue\n","            status = TextCleaningOps.remove_urls(status)\n","            status = TextCleaningOps.normalize_apostrophs(status)\n","            status = TextCleaningOps.remove_null_chars(status)\n","            status = re.sub(r' +', ' ', status)\n","            status = TextCleaningOps.remove_punctuation_signs(status)\n","            status = TextCleaningOps.flatten_to_ascii(status)\n","            status = re.sub(r' +', ' ', status)\n","            cleaned_lines[k] = status\n","        return cleaned_lines\n","\n","    @staticmethod\n","    def do_all_cleaning_ops_with_optional_ascii(map_of_lines, remove_non_ascii=False):\n","        cleaned_lines = {}\n","        if map_of_lines is None:\n","            return cleaned_lines\n","        for k, status in map_of_lines.items():\n","            if status is None or not status.strip():\n","                cleaned_lines[k] = \"\"\n","                continue\n","            status = TextCleaningOps.remove_urls(status)\n","            status = TextCleaningOps.normalize_apostrophs(status)\n","            status = TextCleaningOps.remove_null_chars(status)\n","            status = re.sub(r' +', ' ', status)\n","            status = TextCleaningOps.remove_punctuation_signs(status)\n","            if remove_non_ascii:\n","                status = TextCleaningOps.flatten_to_ascii(status)\n","            status = re.sub(r' +', ' ', status)\n","            cleaned_lines[k] = status\n","        return cleaned_lines\n","\n","    @staticmethod\n","    def do_all_cleaning_ops_string(status):\n","        if status is None:\n","            return \"\"\n","        status = TextCleaningOps.remove_urls(status)\n","        status = TextCleaningOps.normalize_apostrophs(status)\n","        status = TextCleaningOps.remove_null_chars(status)\n","        status = re.sub(r' +', ' ', status)\n","        status = TextCleaningOps.remove_punctuation_signs(status)\n","        status = TextCleaningOps.flatten_to_ascii(status)\n","        status = re.sub(r' +', ' ', status)\n","        return status\n","\n","    @staticmethod\n","    def do_all_cleaning_ops_set(lines):\n","        if lines is None:\n","            return set()\n","        map_of_lines = {i: line for i, line in enumerate(lines)}\n","        cleaned_map = TextCleaningOps.do_all_cleaning_ops(map_of_lines)\n","        return set(cleaned_map.values())\n"]},{"cell_type":"code","execution_count":64,"id":"c184db8e","metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1725921393533,"user":{"displayName":"Aditya Singhal","userId":"02233461186993627837"},"user_tz":240},"id":"c184db8e"},"outputs":[],"source":["class CombinationGenerator:\n","    def __init__(self, n: int, r: int):\n","        if r > n:\n","            raise ValueError(\"r cannot be greater than n\")\n","        if n < 1:\n","            raise ValueError(\"n must be at least 1\")\n","\n","        self.n = n\n","        self.r = r\n","        self.a = list(range(r))\n","        self.total = Decimal(factorial(n)) / (Decimal(factorial(r)) * Decimal(factorial(n - r)))\n","        self.num_left = self.total\n","        self.reset()\n","\n","    def reset(self):\n","        self.a = list(range(self.r))\n","        self.num_left = Decimal(self.total)\n","\n","    def get_num_left(self) -> Decimal:\n","        return self.num_left\n","\n","    def has_more(self) -> bool:\n","        return self.num_left > 0\n","\n","    def get_total(self) -> Decimal:\n","        return self.total\n","\n","    def get_next(self) -> List[int]:\n","        if self.num_left == self.total:\n","            self.num_left -= 1\n","            return self.a\n","\n","        i = self.r - 1\n","        while self.a[i] == self.n - self.r + i:\n","            i -= 1\n","\n","        self.a[i] += 1\n","        for j in range(i + 1, self.r):\n","            self.a[j] = self.a[i] + j - i\n","\n","        self.num_left -= 1\n","        return self.a\n"]},{"cell_type":"code","execution_count":65,"id":"bf59b30e","metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1725921393533,"user":{"displayName":"Aditya Singhal","userId":"02233461186993627837"},"user_tz":240},"id":"bf59b30e"},"outputs":[],"source":["class PatternOfInterestChecker:\n","    def __init__(self):\n","        self.patterns_of_interest: Set[PatternOfInterest] = set()\n","\n","    def load_patterns_of_interest(self, file_path: str):\n","        try:\n","            with open(file_path, mode='r', encoding='utf-8') as file:\n","                reader = csv.reader(file, delimiter='\\t')\n","                next(reader)  # Skip header row\n","                for row in reader:\n","                    poi = PatternOfInterest()\n","                    poi.set_description(row[0])\n","                    poi.set_regex(row[1])\n","                    poi.set_should_apply_to_lower_case_text(row[2].lower() == 'true')\n","\n","                    category_ids = row[3].split(',')\n","                    categories = [Category(cat_id) for cat_id in category_ids]\n","                    poi.set_categories(categories)\n","\n","                    # Assuming type_of_text_fragment can be None or a valid string\n","                    poi.set_type_of_text_fragment(row[4] if row[4] else \"stop\")\n","\n","                    self.patterns_of_interest.add(poi)\n","        except IOError:\n","            print(\"Error when loading patterns in tokenizer\")\n","\n","    def contains_percentage(self, text: str) -> Optional[str]:\n","        # Do we find a percentage?\n","        if re.search(r'\\d%', text):\n","            # If so, is it followed by \"off\"?\n","            if re.search(r'\\d% (off|cash back)', text, re.IGNORECASE):\n","                return \"611\"\n","            else:\n","                return \"621\"\n","        return None\n","\n","    def returns_match_or_not(self, text: str) -> PatternOfInterest:\n","        for poi in self.patterns_of_interest:\n","            if poi.get_pattern() and poi.get_pattern().fullmatch(text):\n","                to_return = PatternOfInterest()\n","                to_return.set_categories(poi.get_categories())\n","                to_return.set_type_of_text_fragment_enum(poi.get_type_of_text_fragment_enum())\n","                to_return.set_description(poi.get_description())\n","                to_return.set_matched(True)\n","                return to_return\n","\n","        poi = PatternOfInterest()\n","        poi.set_matched(False)\n","        return poi\n"]},{"cell_type":"code","execution_count":66,"id":"906b8572","metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1725921393533,"user":{"displayName":"Aditya Singhal","userId":"02233461186993627837"},"user_tz":240},"id":"906b8572"},"outputs":[],"source":["class UmigonTokenizer:\n","    initialized = False\n","    poi_checker = None\n","\n","    class CurrentFragment(Enum):\n","        WHITE_SPACE = 1\n","        PUNCTUATION = 2\n","        NON_WORD = 3\n","        TERM = 4\n","        NOT_STARTED = 5\n","\n","    @staticmethod\n","    def initialize(file_path: Optional[str] = None):\n","        if not UmigonTokenizer.initialized:\n","            UmigonTokenizer.poi_checker = PatternOfInterestChecker()\n","            if file_path:\n","                UmigonTokenizer.poi_checker.load_patterns_of_interest(file_path)\n","            UmigonTokenizer.initialized = True\n","\n","    @staticmethod\n","    def tokenize(text: str, language_specific_lexicon: Optional[Set[str]] = None) -> List['TextFragment']:\n","        if not UmigonTokenizer.initialized:\n","            UmigonTokenizer.initialize()\n","\n","        text_fragments = []\n","        if language_specific_lexicon is None:\n","            language_specific_lexicon = set()\n","\n","        text_fragment_started = False\n","        dash_like_characters = {\"-\", \"‐\", \"‑\", \"‒\", \"–\", \"—\", \"︱\", \"﹘\", \"﹣\", \"－\", \"_\", \"\\\\\", \"/\", \"|\"}\n","\n","        curr_fragment = UmigonTokenizer.CurrentFragment.NOT_STARTED\n","\n","        white_space = None\n","        term = None\n","        punctuation = None\n","        non_word = None\n","        emoji = None\n","\n","        code_points = [ord(c) for c in text]\n","\n","        for index, code_point in enumerate(code_points):\n","            char = chr(code_point)\n","\n","            is_white_space = char.isspace()\n","            is_emoji = char in EMOJI_DATA\n","            is_punctuation = bool(re.match(r\"[^\\w\\s]\", char))\n","\n","            if curr_fragment == UmigonTokenizer.CurrentFragment.WHITE_SPACE:\n","                if is_white_space:\n","                    white_space.add_string_to_original_form(char)\n","                    if char == \"\\n\":\n","                        white_space.set_sentence_or_line_break(True)\n","                else:\n","                    text_fragments.append(white_space)\n","                    text_fragment_started = False\n","                    curr_fragment = UmigonTokenizer.CurrentFragment.NOT_STARTED\n","\n","            elif curr_fragment == UmigonTokenizer.CurrentFragment.TERM:\n","                if is_white_space or is_emoji or is_punctuation:\n","                    UmigonTokenizer.process_term(term, language_specific_lexicon, text_fragments)\n","                    text_fragment_started = False\n","                    curr_fragment = UmigonTokenizer.CurrentFragment.NOT_STARTED\n","                else:\n","                    term.add_string_to_original_form(char)\n","\n","            elif curr_fragment == UmigonTokenizer.CurrentFragment.NON_WORD:\n","                if is_white_space or is_emoji:\n","                    text_fragments.append(non_word)\n","                    text_fragment_started = False\n","                    curr_fragment = UmigonTokenizer.CurrentFragment.NOT_STARTED\n","                    non_word = NonWord()\n","                else:\n","                    curr_non_word = non_word.get_original_form() + char\n","                    if UmigonTokenizer.poi_checker.returns_match_or_not(curr_non_word).matched:\n","                        non_word.add_string_to_original_form(char)\n","                    else:\n","                        text_fragments.append(non_word)\n","                        text_fragment_started = False\n","                        curr_fragment = UmigonTokenizer.CurrentFragment.NOT_STARTED\n","                        non_word = NonWord()\n","\n","            elif curr_fragment == UmigonTokenizer.CurrentFragment.PUNCTUATION:\n","                if is_punctuation:\n","                    punctuation.add_string_to_original_form(char)\n","                else:\n","                    pattern_of_interest = UmigonTokenizer.poi_checker.returns_match_or_not(punctuation.get_original_form())\n","                    if pattern_of_interest.matched:\n","                        non_word = punctuation.to_non_word(pattern_of_interest, punctuation.get_original_form())\n","                        text_fragments.append(non_word)\n","                    else:\n","                        text_fragments.append(punctuation)\n","                    text_fragment_started = False\n","                    curr_fragment = UmigonTokenizer.CurrentFragment.NOT_STARTED\n","\n","            if not text_fragment_started:\n","                if is_white_space:\n","                    text_fragment_started = True\n","                    white_space = WhiteSpace()\n","                    white_space.index_cardinal = index\n","                    white_space.index_ordinal = len(text_fragments)\n","                    white_space.add_string_to_original_form(char)\n","                    if char == \"\\n\":\n","                        white_space.set_sentence_or_line_break(True)\n","                    curr_fragment = UmigonTokenizer.CurrentFragment.WHITE_SPACE\n","\n","                elif is_emoji:\n","                    text_fragment_started = True\n","                    emoji = Emoji()\n","                    emoji.index_cardinal = index\n","                    emoji.index_ordinal = len(text_fragments)\n","                    emoji.add_string_to_original_form(char)\n","                    text_fragments.append(emoji)\n","                    curr_fragment = UmigonTokenizer.CurrentFragment.NOT_STARTED\n","\n","                elif is_punctuation:\n","                    text_fragment_started = True\n","                    punctuation = Punctuation()\n","                    punctuation.index_cardinal = index\n","                    punctuation.index_ordinal = len(text_fragments)\n","                    punctuation.add_string_to_original_form(char)\n","                    curr_fragment = UmigonTokenizer.CurrentFragment.PUNCTUATION\n","\n","                elif not is_white_space and not is_punctuation:\n","                    text_fragment_started = True\n","                    term = Term()\n","                    term.index_cardinal = index\n","                    term.index_ordinal = len(text_fragments)\n","                    term.add_string_to_original_form(char)\n","                    curr_fragment = UmigonTokenizer.CurrentFragment.TERM\n","\n","                elif not is_punctuation and is_white_space:\n","                    non_word = NonWord()\n","                    non_word.index_cardinal = index\n","                    non_word.index_ordinal = len(text_fragments)\n","                    non_word.add_string_to_original_form(char)\n","                    curr_fragment = UmigonTokenizer.CurrentFragment.NON_WORD\n","\n","        # Process remaining fragment\n","        if curr_fragment == UmigonTokenizer.CurrentFragment.TERM:\n","            UmigonTokenizer.process_term(term, language_specific_lexicon, text_fragments)\n","\n","        elif curr_fragment == UmigonTokenizer.CurrentFragment.WHITE_SPACE:\n","            text_fragments.append(white_space)\n","\n","        elif curr_fragment == UmigonTokenizer.CurrentFragment.PUNCTUATION:\n","            pattern_of_interest = UmigonTokenizer.poi_checker.returns_match_or_not(punctuation.get_original_form())\n","            if pattern_of_interest.matched:\n","                non_word = punctuation.to_non_word(pattern_of_interest, punctuation.get_original_form())\n","                text_fragments.append(non_word)\n","            else:\n","                text_fragments.append(punctuation)\n","\n","        elif curr_fragment == UmigonTokenizer.CurrentFragment.NON_WORD:\n","            text_fragments.append(non_word)\n","\n","        return text_fragments\n","\n","    @staticmethod\n","    def process_term(term, language_specific_lexicon, text_fragments):\n","        original_form = term.get_original_form()\n","        cleaned_form = RepeatedCharactersRemover.repeated_characters(original_form, language_specific_lexicon)\n","        cleaned_form = cleaned_form.replace(\"[’ʼ]\", \"'\")\n","        cleaned_and_stripped_form = TextCleaningOps.flatten_to_ascii(cleaned_form)\n","        term.cleaned_form = cleaned_form\n","        term.cleaned_and_stripped_form = cleaned_and_stripped_form\n","\n","        pattern_of_interest = UmigonTokenizer.poi_checker.returns_match_or_not(term.cleaned_and_stripped_form)\n","        if pattern_of_interest.matched:\n","            non_word = NonWord()\n","            non_word.index_cardinal = term.index_cardinal\n","            non_word.index_ordinal = term.index_ordinal\n","            non_word.original_form = term.original_form\n","            non_word.type_of_text_fragment_enum = pattern_of_interest.type_of_text_fragment_enum\n","            non_word.poi = pattern_of_interest\n","            text_fragments.append(non_word)\n","        else:\n","            text_fragments.append(term)\n"]},{"cell_type":"code","execution_count":67,"id":"405ec1f2","metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1725921393533,"user":{"displayName":"Aditya Singhal","userId":"02233461186993627837"},"user_tz":240},"id":"405ec1f2"},"outputs":[],"source":["class DataManager:\n","    def __init__(self):\n","        self.original_strings_per_line: Dict[int, str] = {}\n","        self.text_fragments_per_line: Dict[int, List[TextFragment]] = defaultdict(list)\n","        self.cleaned_and_stripped_ngrams_per_line: Dict[int, Set[str]] = defaultdict(set)\n","        self.list_of_n_grams_global: List[NGram] = []\n","        self.n_grams_and_global_count: Dict[NGram, int] = defaultdict(int)\n","        self.stringified_cleaned_and_stripped_ngram_to_lemmatized_form: Dict[str, str] = {}\n","        self.mapping_non_lemmatized_form_to_ngram: Dict[str, NGram] = {}\n","        self.mapping_lemmatized_form_to_ngram: Dict[str, NGram] = {}\n","\n","    # Getters and Setters\n","    def get_original_strings_per_line(self) -> Dict[int, str]:\n","        return self.original_strings_per_line\n","\n","    def set_map_of_lines(self, original_strings_per_line: Dict[int, str]):\n","        self.original_strings_per_line = original_strings_per_line\n","\n","    def get_text_fragments_per_line(self) -> Dict[int, List[TextFragment]]:\n","        return self.text_fragments_per_line\n","\n","    def set_text_fragments_per_line(self, text_fragments_per_line: Dict[int, List[TextFragment]]):\n","        self.text_fragments_per_line = text_fragments_per_line\n","\n","    def get_cleaned_and_stripped_ngrams_per_line(self) -> Dict[int, Set[str]]:\n","        return self.cleaned_and_stripped_ngrams_per_line\n","\n","    def set_cleaned_and_stripped_ngrams_per_line(self, cleaned_and_stripped_ngrams_per_line: Dict[int, Set[str]]):\n","        self.cleaned_and_stripped_ngrams_per_line = cleaned_and_stripped_ngrams_per_line\n","\n","    def get_list_of_n_grams_global(self) -> List[NGram]:\n","        return self.list_of_n_grams_global\n","\n","    def set_list_of_n_grams_global(self, list_of_n_grams_global: List[NGram]):\n","        self.list_of_n_grams_global = list_of_n_grams_global\n","\n","    def get_stringified_cleaned_and_stripped_ngram_to_lemmatized_form(self) -> Dict[str, str]:\n","        return self.stringified_cleaned_and_stripped_ngram_to_lemmatized_form\n","\n","    def set_stringified_cleaned_and_stripped_ngram_to_lemmatized_form(self, stringified_cleaned_and_stripped_ngram_to_lemmatized_form: Dict[str, str]):\n","        self.stringified_cleaned_and_stripped_ngram_to_lemmatized_form = stringified_cleaned_and_stripped_ngram_to_lemmatized_form\n","\n","    def get_mapping_non_lemmatized_form_to_ngram(self) -> Dict[str, NGram]:\n","        return self.mapping_non_lemmatized_form_to_ngram\n","\n","    def set_mapping_non_lemmatized_form_to_ngram(self, mapping_non_lemmatized_form_to_ngram: Dict[str, NGram]):\n","        self.mapping_non_lemmatized_form_to_ngram = mapping_non_lemmatized_form_to_ngram\n","\n","    def get_mapping_lemmatized_form_to_ngram(self) -> Dict[str, NGram]:\n","        return self.mapping_lemmatized_form_to_ngram\n","\n","    def set_mapping_lemmatized_form_to_ngram(self, mapping_lemmatized_form_to_ngram: Dict[str, NGram]):\n","        self.mapping_lemmatized_form_to_ngram = mapping_lemmatized_form_to_ngram\n","\n","    def get_n_grams_and_global_count(self) -> Dict[NGram, int]:\n","        return self.n_grams_and_global_count\n","\n","    def set_n_grams_and_global_count(self, n_grams_and_global_count: Dict[NGram, int]):\n","        self.n_grams_and_global_count = n_grams_and_global_count\n"]},{"cell_type":"code","execution_count":68,"id":"cb3dab5d","metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1725921393533,"user":{"displayName":"Aditya Singhal","userId":"02233461186993627837"},"user_tz":240},"id":"cb3dab5d"},"outputs":[],"source":["class Cooc:\n","    def __init__(self, a: NGram = None, b: NGram = None):\n","        if a is not None and b is not None:\n","            if a.get_original_form_lemmatized().lower() > b.get_original_form_lemmatized().lower():\n","                self.a = a\n","                self.b = b\n","            else:\n","                self.a = b\n","                self.b = a\n","        else:\n","            self.a = a\n","            self.b = b\n","\n","    def get_a(self) -> NGram:\n","        return self.a\n","\n","    def set_a(self, a: NGram):\n","        self.a = a\n","\n","    def get_b(self) -> NGram:\n","        return self.b\n","\n","    def set_b(self, b: NGram):\n","        self.b = b\n","\n","    def __hash__(self) -> int:\n","        hash_first = hash(self.a.get_original_form_lemmatized())\n","        hash_second = hash(self.b.get_original_form_lemmatized())\n","        max_hash = max(hash_first, hash_second)\n","        min_hash = min(hash_first, hash_second)\n","        return min_hash * 31 + max_hash\n","\n","    def __eq__(self, other: Any) -> bool:\n","        if not isinstance(other, Cooc):\n","            return False\n","        pair_o = other\n","        return ((self.a.get_original_form_lemmatized().lower() == pair_o.a.get_original_form_lemmatized().lower()\n","                 and self.b.get_original_form_lemmatized().lower() == pair_o.b.get_original_form_lemmatized().lower())\n","                or (self.a.get_original_form_lemmatized().lower() == pair_o.b.get_original_form_lemmatized().lower()\n","                    and self.b.get_original_form_lemmatized().lower() == pair_o.a.get_original_form_lemmatized().lower()))\n","\n","    def __str__(self) -> str:\n","        return f\"{self.a.get_original_form_lemmatized()}{{--}}{self.b.get_original_form_lemmatized()}\"\n"]},{"cell_type":"code","execution_count":69,"id":"75c3329d","metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1725921393533,"user":{"displayName":"Aditya Singhal","userId":"02233461186993627837"},"user_tz":240},"id":"75c3329d"},"outputs":[],"source":["class PerformCombinationsOnNGrams:\n","    def __init__(self, table: List[NGram]):\n","        self.table = table\n","\n","    def call(self) -> List[Cooc]:\n","        list_of_coocs = []\n","\n","        # Find all pairs (2) of the n-grams\n","        x = CombinationGenerator(len(self.table), 2)\n","\n","        while x.has_more():\n","            indices = x.get_next()\n","            cooc = Cooc(self.table[indices[0]], self.table[indices[1]])\n","            list_of_coocs.append(cooc)\n","\n","        return list_of_coocs\n"]},{"cell_type":"code","execution_count":70,"id":"a1d61e72","metadata":{"executionInfo":{"elapsed":158,"status":"ok","timestamp":1725921393688,"user":{"displayName":"Aditya Singhal","userId":"02233461186993627837"},"user_tz":240},"id":"a1d61e72"},"outputs":[],"source":["class LemmatizerInterface:\n","    def lemmatize_term(self, term: str) -> str:\n","        raise NotImplementedError(\"Subclasses should implement this method\")"]},{"cell_type":"code","execution_count":71,"id":"19c7be22","metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1725921393688,"user":{"displayName":"Aditya Singhal","userId":"02233461186993627837"},"user_tz":240},"id":"19c7be22"},"outputs":[],"source":["class BritishAmericanMerger:\n","    dont_merge = {\"our\", \"flour\", \"four\", \"hour\", \"pour\", \"sour\", \"tour\"}\n","\n","    @staticmethod\n","    def merge_to_american_english(input_string):\n","        if input_string in BritishAmericanMerger.dont_merge:\n","            return input_string\n","\n","        if input_string.endswith(\"our\") and not any(input_string.endswith(suffix) for suffix in [\"tour\", \"hour\", \"pour\"]):\n","            return input_string[:-3] + \"or\"\n","        else:\n","            return input_string\n"]},{"cell_type":"code","execution_count":72,"id":"0447fd1d","metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1725921393689,"user":{"displayName":"Aditya Singhal","userId":"02233461186993627837"},"user_tz":240},"id":"0447fd1d"},"outputs":[],"source":["\n","class LemmatizerEN(LemmatizerInterface):\n","\n","    def __init__(self, merge_to_american_english):\n","        self.merge_to_american_english = merge_to_american_english\n","\n","    def dont_merge_to_american_english(self):\n","        self.merge_to_american_english = False\n","\n","    def lemmatize_term(self, term):\n","        if (term.endswith(\"s\") or term.endswith(\"s'\")) and not (\n","            term.endswith(\"us\") or\n","            term.endswith(\"as\") or\n","            term.endswith(\"ss\") or\n","            term.endswith(\"sses\") or\n","            term.endswith(\"ies\") or\n","            term.endswith(\"is\")\n","        ):\n","            if term.endswith(\"s\"):\n","                term = term[:-1]\n","            if term.endswith(\"s'\"):\n","                term = term[:-2]\n","        elif term.endswith(\"'\"):\n","            term = term[:-1]\n","\n","        if term.endswith(\"sses\"):\n","            term = term[:-2]\n","        if term.endswith(\"ies\"):\n","            if not term.endswith(\"movies\"):\n","                term = term[:-3] + \"y\"\n","            else:\n","                term = term[:-1]\n","        elif term.endswith(\"'s\"):\n","            term = term[:-2]\n","        elif term.endswith(\"ed\"):\n","            if (\n","                term.endswith(\"rred\") or\n","                term.endswith(\"mmed\")\n","            ):\n","                term = term[:-3]\n","            elif (\n","                term.endswith(\"lked\") or\n","                term.endswith(\"cked\") or\n","                term.endswith(\"pted\") or\n","                term.endswith(\"ssed\") or\n","                term.endswith(\"lled\") or\n","                term.endswith(\"iased\") or\n","                (term.endswith(\"red\") and not (term.endswith(\"ired\") or term.endswith(\"ured\") or term == \"clustered\" or term.endswith(\"ared\"))) or\n","                (term.endswith(\"med\") and not (term.endswith(\"framed\") or term.endswith(\"lamed\") or term.endswith(\"named\") or term.endswith(\"shamed\"))) or\n","                term.endswith(\"aired\") or\n","                term.endswith(\"used\") or\n","                term.endswith(\"ned\") or\n","                (term.endswith(\"ded\") and not term.endswith(\"ided\")) or\n","                (term.endswith(\"ted\") and not term.endswith(\"ated\"))\n","            ):\n","                term = term[:-2]\n","            elif term.endswith(\"ied\"):\n","                term = term[:-3] + \"y\"\n","            elif term.endswith(\"eed\"):\n","                pass  # do nothing (as in: exceed, proceed)\n","            else:\n","                term = term[:-1]  # purchased -> purchase\n","        elif term.endswith(\"ing\"):\n","            if term.endswith(\"king\"):\n","                term = term[:-3] + \"e\"\n","            elif term.endswith(\"ging\") and not term.endswith(\"gging\"):\n","                term = term[:-3] + \"e\"\n","            elif (\n","                term.endswith(\"sing\") or\n","                term.endswith(\"zing\") or\n","                term.endswith(\"cing\") or\n","                (term.endswith(\"oding\") and not term.endswith(\"ooding\")) or\n","                (term.endswith(\"ting\") and not (term.endswith(\"sting\") or term.endswith(\"tting\") or term.endswith(\"nting\") or term.endswith(\"cting\"))) or\n","                (term.endswith(\"ming\") and (term.endswith(\"framing\") or not (term.endswith(\"laming\") or term.endswith(\"naming\") or term.endswith(\"shaming\")))) or\n","                term.endswith(\"ving\") or\n","                term.endswith(\"ring\") and not term.endswith(\"during\")\n","            ):\n","                term = term[:-3] + \"e\"\n","            elif len(term) > 2:\n","                term = term[:-3]\n","                # running has become runn. Should become run.\n","                size = len(term)\n","                if size > 1:\n","                    last_two_letters = term[-2:]\n","                    if last_two_letters[0] == last_two_letters[1]:\n","                        term = term[:-1]\n","                    size = len(term)\n","                    if size > 1:\n","                        # voting has become vot. Should become vote.\n","                        # Same for any word ending in at or ot (not plant, suspect, ...).\n","                        # Yet, it will miscorrect \"pivot\" and \"format\"\n","                        last_two_letters = term[-2:]\n","                        if term not in [\"pivot\", \"format\"]:\n","                            if last_two_letters in [\"at\", \"ot\", \"id\"]:\n","                                term = term + \"e\"\n","        elif term.endswith(\"ier\"):\n","            term = term[:-3] + \"y\"\n","\n","        if self.merge_to_american_english:\n","            term = BritishAmericanMerger.merge_to_american_english(term)\n","\n","        return term\n"]},{"cell_type":"code","execution_count":73,"id":"4d47b2a3","metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1725921393689,"user":{"displayName":"Aditya Singhal","userId":"02233461186993627837"},"user_tz":240},"id":"4d47b2a3"},"outputs":[],"source":["class Lemmatizer:\n","    def __init__(self, lang: str):\n","        self.no_lemma_en = {\n","            \"access\", \"accumbens\", \"addresses\", \"afterwards\", \"always\", \"amazing\",\n","            \"approaches\", \"analyses\", \"biases\", \"businesses\", \"ceiling\", \"classes\",\n","            \"crises\", \"daunting\", \"discusses\", \"during\", \"economics\", \"elsevier\",\n","            \"ethics\", \"focuses\", \"fries\", \"goes\", \"humanities\", \"hundred\",\n","            \"hypotheses\", \"inches\", \"king\", \"lens\", \"linguistics\", \"lies\",\n","            \"losses\", \"marketing\", \"morning\", \"news\", \"outlier\", \"outstanding\",\n","            \"physics\", \"politics\", \"premises\", \"processes\", \"red\", \"rigged\", \"ries\",\n","            \"series\", \"sometimes\", \"something\", \"species\", \"spring\", \"status\",\n","            \"ted\", \"themselves\", \"neural processes\", \"united\", \"wales\", \"witnesses\"\n","        }\n","\n","        self.no_lemma_fr = {\n","            \"accès\", \"alors\", \"alpes\", \"ailleurs\", \"apres\", \"après\", \"aupres\", \"auprès\",\n","            \"Calvados\", \"concours\", \"corps\", \"cours\", \"dans\", \"discours\", \"divers\", \"etes\",\n","            \"êtes\", \"ethos\", \"éthos\", \"gens\", \"gros\", \"lors\", \"outils\", \"pays\", \"parcours\",\n","            \"pres\", \"près\", \"proces\", \"procès\", \"propos\", \"puis\", \"sans\", \"secours\", \"sens\",\n","            \"sommes\", \"succès\", \"succes\", \"temps\", \"toujours\", \"travers\", \"très\", \"tres\",\n","            \"univers\", \"viens\", \"vos\"\n","        }\n","\n","        self.no_lemma_es = {\n","            \"revés\", \"atrás\", \"país\", \"gafas\", \"años\", \"adiós\", \"peces\", \"tres\", \"azulgris\",\n","            \"compás\", \"menos\", \"mes\", \"tijeras\", \"avis\", \"anís\", \"vals\", \"compás\", \"alas\",\n","            \"análisis\", \"oasis\", \"paréntesis\", \"estrés\", \"colchones\", \"espejuelos\", \"martes\",\n","            \"lunes\", \"miércoles\", \"jueves\", \"viernes\", \"calcetines\", \"álbumes\", \"nueces\", \"veces\",\n","            \"coches\", \"alfileres\", \"lazos\", \"pistaches\", \"pañales\", \"prismas\", \"bolsos\", \"panes\",\n","            \"alfileres\", \"golpes\", \"jardines\", \"manos\", \"ojos\", \"dedos\", \"radios\"\n","        }\n","\n","        self.no_lemma = {\n","            \"analytics\", \"accumbens\", \"aws\", \"bayes\", \"business\", \"charles\", \"ects\", \"cnrs\",\n","            \"cosmos\", \"cowles\", \"deep learning\", \"developer\", \"ethos\", \"faas\", \"forbes\",\n","            \"iaas\", \"james\", \"keynes\", \"koopmans\", \"nhs\", \"paas\", \"paris\", \"programming\",\n","            \"reactjs\", \"saas\", \"siemens\", \"sanders\", \"ted\", \"virus\", \"vuejs\", \"united states\"\n","        }\n","\n","        self.no_lemma_set = set(self.no_lemma)\n","\n","        self.merge_to_american_english = True\n","        self.lemmatizer_interface = None\n","\n","        if lang == \"en\":\n","            self.no_lemma_set = set(self.no_lemma_en)\n","            self.lemmatizer_interface = LemmatizerEN(self.merge_to_american_english)\n","        elif lang == \"fr\":\n","            self.no_lemma_set = set(self.no_lemma_fr)\n","            self.lemmatizer_interface = LemmatizerFR()\n","        elif lang == \"es\":\n","            self.no_lemma_set = set(self.no_lemma_es)\n","            self.lemmatizer_interface = LemmatizerES()\n","        else:\n","            self.no_lemma_set = set(self.no_lemma_en)\n","            self.lemmatizer_interface = LemmatizerEN(self.merge_to_american_english)\n","\n","    def dont_merge_to_american_english(self):\n","        self.merge_to_american_english = False\n","        if isinstance(self.lemmatizer_interface, LemmatizerEN):\n","            self.lemmatizer_interface.dont_merge_to_american_english()\n","\n","    def lemmatize(self, term: str) -> str:\n","        if term in self.no_lemma_set:\n","            return term\n","        term = self.lemmatizer_interface.lemmatize_term(term)\n","\n","        if term.endswith(\"'\"):\n","            term = term[:-1]\n","        return term.strip()\n","\n","    def sentence_lemmatizer(self, sentence: str) -> str:\n","        terms = sentence.split()\n","        return \" \".join(self.lemmatizer_interface.lemmatize_term(term) for term in terms).strip()"]},{"cell_type":"code","execution_count":74,"id":"q38s36CwAWzl","metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1725921393689,"user":{"displayName":"Aditya Singhal","userId":"02233461186993627837"},"user_tz":240},"id":"q38s36CwAWzl"},"outputs":[],"source":["class CurvedEdge(FancyArrowPatch):\n","    def __init__(self, p1, p2, rad=0.2, **kwargs):\n","        super().__init__(p1, p2, connectionstyle=f\"arc3,rad={rad}\", **kwargs)\n","\n","    @staticmethod\n","    def interpolate_color(color1, color2, alpha):\n","        \"\"\"Interpolate between two colors with a given alpha.\"\"\"\n","        try:\n","            c1 = np.array(to_rgba(color1))\n","            c2 = np.array(to_rgba(color2))\n","            interpolated_color = c1 * (1 - alpha) + c2 * alpha\n","            return tuple(interpolated_color)\n","        except Exception as e:\n","            print(f\"Error in color interpolation: {e}\")\n","            return to_rgba(color1)  # Fallback to color1 if there's an error\n","\n","    @staticmethod\n","    def get_cluster_colors(seed, num_clusters):\n","        \"\"\"Generate a deterministic list of colors for clusters based on a seed.\"\"\"\n","        np.random.seed(seed)\n","        cmap = plt.get_cmap('tab10')\n","        num_colors = cmap.N\n","        # Generate a list of colors, cycling through the colormap if needed\n","        colors = [cmap(i % num_colors) for i in range(num_clusters)]\n","        return colors\n","\n","    @staticmethod\n","    def draw_curved_edges(G, pos, ax, rad=0.2, edge_colors=None, linewidth=1):\n","        if edge_colors is None:\n","            edge_colors = ['black'] * len(G.edges())\n","\n","        for (u, v), color in zip(G.edges(), edge_colors):\n","            p1, p2 = pos[u], pos[v]\n","            arrow = CurvedEdge(p1, p2, rad=rad, color=color, linewidth=linewidth)\n","            ax.add_patch(arrow)\n","\n","    @staticmethod\n","    def visualize_graph_to_variable(G, seed):\n","        pos = nx.spring_layout(G, seed=seed)\n","\n","        # Use 'size' attribute to set node sizes, scaled up for visibility\n","        node_sizes = [G.nodes[node]['weights']['countTerms'] * 10 for node in G.nodes()]  # Increased scaling factor\n","\n","        # Define a color map for clusters with seeding\n","        num_clusters = len(set(nx.get_node_attributes(G, 'cluster').values()))\n","        cluster_colors = CurvedEdge.get_cluster_colors(seed=42, num_clusters=num_clusters)  # Seed for reproducibility\n","        cluster_ids = set(nx.get_node_attributes(G, 'cluster').values())\n","        cluster_color_map = {cluster_id: cluster_colors[i % len(cluster_colors)] for i, cluster_id in enumerate(cluster_ids)}\n","\n","        # Get node colors based on cluster IDs\n","        node_colors = [cluster_color_map[G.nodes[node]['cluster']] for node in G.nodes()]\n","\n","        # Define edge colors based on clusters of source and target nodes\n","        edge_colors = []\n","        for u, v in G.edges():\n","            cluster_u = G.nodes[u]['cluster']\n","            cluster_v = G.nodes[v]['cluster']\n","            if cluster_u != cluster_v:\n","                # Create gradient color between two clusters\n","                color_u = cluster_color_map[cluster_u]\n","                color_v = cluster_color_map[cluster_v]\n","                edge_color = CurvedEdge.interpolate_color(color_u, color_v, alpha=0.5)\n","            else:\n","                edge_color = cluster_color_map[cluster_u]\n","            edge_colors.append(edge_color)\n","\n","        fig, ax = plt.subplots(figsize=(12, 12))  # Increased figure size for better clarity\n","\n","        # Adjust margins\n","        plt.subplots_adjust(left=0.1, right=0.9, top=0.9, bottom=0.1)\n","\n","        # Draw nodes\n","        nx.draw_networkx_nodes(G, pos, ax=ax, node_size=node_sizes, node_color=node_colors, edgecolors='k', alpha=0.6)\n","\n","        # Draw labels (ensure labels are visible)\n","        labels = nx.get_node_attributes(G, 'label')\n","        nx.draw_networkx_labels(G, pos, labels=labels, ax=ax, font_size=10, font_color='black', bbox=dict(facecolor='white', alpha=0.2, edgecolor='none', pad=1))\n","\n","        # Draw curved edges with color based on cluster IDs\n","        CurvedEdge.draw_curved_edges(G, pos, ax, rad=0.3, edge_colors=edge_colors, linewidth=2)\n","\n","        plt.title(\"Network Visualization\", fontsize=15)\n","        plt.axis('off')  # Turn off the axis\n","        return fig  # Return the figure object\n","\n","    @staticmethod\n","    def create_graph_from_json(json_data):\n","        # Ensure json_data is a dictionary\n","        if isinstance(json_data, str):\n","            json_data = json.loads(json_data)\n","\n","        G = nx.Graph()\n","\n","        # Extract nodes and add cluster and size attributes\n","        for item in json_data['network']['items']:\n","            G.add_node(item['id'],\n","                      label=item['label'],\n","                      cluster=item['cluster'],\n","                      x=item['x'],\n","                      y=item['y'],\n","                      weights=item['weights'],\n","                      scores=item['scores'])\n","\n","        # Extract edges\n","        for link in json_data['network']['links']:\n","            G.add_edge(link['source_id'], link['target_id'], weight=link['strength'])\n","\n","        return G\n"]},{"cell_type":"code","execution_count":75,"id":"ZRPkUtVwf0tF","metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1725921393689,"user":{"displayName":"Aditya Singhal","userId":"02233461186993627837"},"user_tz":240},"id":"ZRPkUtVwf0tF"},"outputs":[],"source":["def calculate_node_sizes(G):\n","    \"\"\"Calculate node sizes based on the sum of link strengths.\"\"\"\n","    node_sizes = {}\n","    for node in G.nodes():\n","        total_strength = sum(data['weight'] for _, _, data in G.edges(node, data=True))\n","        node_sizes[node] = total_strength\n","    return node_sizes\n","\n","def apply_mds(similarity_matrix):\n","    \"\"\"Apply MDS to reduce similarity matrix to 2D.\"\"\"\n","    mds = MDS(n_components=2, dissimilarity='precomputed', random_state=42, normalized_stress='auto')\n","    dissimilarity_matrix = np.max(similarity_matrix) - similarity_matrix\n","    pos = mds.fit_transform(dissimilarity_matrix)\n","    return pos\n","\n","def create_similarity_matrix(G):\n","    \"\"\"Create a similarity matrix from the graph based on edge weights.\"\"\"\n","    num_nodes = len(G.nodes())\n","    similarity_matrix = np.zeros((num_nodes, num_nodes))\n","\n","    # Node index mapping\n","    node_index = {node: idx for idx, node in enumerate(G.nodes())}\n","\n","    for (u, v, data) in G.edges(data=True):\n","        idx_u = node_index[u]\n","        idx_v = node_index[v]\n","        similarity_matrix[idx_u, idx_v] = data['weight']\n","        similarity_matrix[idx_v, idx_u] = data['weight']  # Symmetric\n","\n","    return similarity_matrix, node_index\n"]}],"metadata":{"colab":{"provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":5}
